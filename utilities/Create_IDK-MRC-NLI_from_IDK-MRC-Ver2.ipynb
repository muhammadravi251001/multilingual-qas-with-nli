{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9171daa4",
   "metadata": {},
   "source": [
    "# Define tool and model of the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41849a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "TOOLS_NAME_NER = \"ner\"\n",
    "MODEL_TOOLS_NAME_NER = \"ageng-anugrah/indobert-large-p2-finetuned-ner\"\n",
    "\n",
    "TOOLS_NAME_POS = \"token-classification\"\n",
    "MODEL_TOOLS_NAME_POS = \"ageng-anugrah/indobert-large-p2-finetuned-chunking\"\n",
    "\n",
    "MODEL_SIMILARITY_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# SAMPLE = sys.maxsize\n",
    "SAMPLE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9fdd4",
   "metadata": {},
   "source": [
    "# Import anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26d313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "import string\n",
    "import contextlib\n",
    "import gc\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback, \n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079cd27c",
   "metadata": {},
   "source": [
    "# Retrieve QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafbf0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset idk_mrc (/root/.cache/huggingface/datasets/idk_mrc/idk_mrc_source/1.0.0/cf468d86fa7341e69998db1449851672ebfb4fa46036929d66b9de15c421334f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fa974fb2de43cc9bab9dcbf9b8b6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3659/3659 [00:16<00:00, 225.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 358/358 [00:01<00:00, 290.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 378/378 [00:01<00:00, 259.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 9332\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 764\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas = conhelps.filtered(lambda x: 'idk_mrc' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas['train'])\n",
    "df_validation = pd.DataFrame(data_qas['validation'])\n",
    "df_test = pd.DataFrame(data_qas['test'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_train['context']))):\n",
    "    for j in df_train[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                           \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                           \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                           ignore_index=True)\n",
    "        else:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": str(), \n",
    "                                                           \"answer_start\": 0, \n",
    "                                                           \"answer_end\": 0}}, \n",
    "                                                           ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_validation['context']))):\n",
    "    for j in df_validation[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)        \n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_test = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_test['context']))):\n",
    "    for j in df_test[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "test_dataset = Dataset.from_dict(new_df_test)\n",
    "\n",
    "data_qas = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})\n",
    "data_qas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae908a",
   "metadata": {},
   "source": [
    "# Convert to NLI, with hypothesis being just do concat question & answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c79ac",
   "metadata": {},
   "source": [
    "## Convert Dataset to DataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b342b8ce-41f9-4714-84a5-1697cfee1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "275dc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qas_train_df = (pd.DataFrame(data_qas[\"train\"])).sample(n=SAMPLE, random_state=42)\n",
    "data_qas_val_df = (pd.DataFrame(data_qas[\"validation\"])).sample(n=SAMPLE, random_state=42)\n",
    "data_qas_test_df = (pd.DataFrame(data_qas[\"test\"])).sample(n=SAMPLE, random_state=42)\n",
    "\n",
    "data_qas_train_df = data_qas_train_df.reset_index(drop=True)\n",
    "data_qas_val_df = data_qas_val_df.reset_index(drop=True)\n",
    "data_qas_test_df = data_qas_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bbf0b",
   "metadata": {},
   "source": [
    "## Retrieve answer text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0424485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_answer_text(data):\n",
    "    for i in range(len(data)):\n",
    "        data['answer'][i] = data['answer'][i]['text']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b1a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qas_train_df = retrieve_answer_text(data_qas_train_df)\n",
    "data_qas_val_df = retrieve_answer_text(data_qas_val_df)\n",
    "data_qas_test_df = retrieve_answer_text(data_qas_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f732909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_qas_train_df[data_qas_train_df['answer'] == '']\n",
    "y = data_qas_val_df[data_qas_val_df['answer'] == '']\n",
    "z = data_qas_test_df[data_qas_test_df['answer'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_wrong_answer_from_unanswerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c40b2536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-ADJP',\n",
       " 1: 'B-ADVP',\n",
       " 2: 'B-INTJ',\n",
       " 3: 'B-NP',\n",
       " 4: 'B-PP',\n",
       " 5: 'B-PRT',\n",
       " 6: 'B-SBAR',\n",
       " 7: 'B-UCP',\n",
       " 8: 'B-VP',\n",
       " 9: 'I-ADJP',\n",
       " 10: 'I-ADVP',\n",
       " 11: 'I-NP',\n",
       " 12: 'I-PP',\n",
       " 13: 'I-SBAR',\n",
       " 14: 'I-UCP',\n",
       " 15: 'I-VP',\n",
       " 16: 'O'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_tools_chunking.model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b177261e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-ORGANISATION',\n",
       " 1: 'B-PERSON',\n",
       " 2: 'B-PLACE',\n",
       " 3: 'I-ORGANISATION',\n",
       " 4: 'I-PERSON',\n",
       " 5: 'I-PLACE',\n",
       " 6: 'O'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_tools_ner.model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returning_answer_form_and_answer_that_suitable(data):\n",
    "    \n",
    "    for i in range(len(data['question']))\n",
    "        \n",
    "        question = data['question'][i]\n",
    "\n",
    "        if \"apa\" in question.split():\n",
    "            answer_form = \"sentence\"\n",
    "            answer_ner = \"\"\n",
    "        \n",
    "        elif \"siapa\" in question.split():\n",
    "            answer_form = \"word\"\n",
    "            answer_ner = \"\"\n",
    "        \n",
    "        elif \"kapan\" in question.split():\n",
    "            answer_form = \"word\"\n",
    "            answer_ner = \"\"\n",
    "        \n",
    "        elif \"dimana\" in question.split():\n",
    "            answer_form = \"word\"\n",
    "            answer_ner = \"\"\n",
    "        \n",
    "        elif \"mengapa\" in question.split():\n",
    "            answer_form = \"sentence\"\n",
    "            answer_ner = \"\"\n",
    "        \n",
    "        elif \"bagaimana\" in question.split():\n",
    "            answer_form = \"sentence\"\n",
    "            answer_ner = \"\"\n",
    "        \n",
    "        elif \"berapa\" in question.split():\n",
    "            answer_form = \"word\"\n",
    "            answer_ner = \"\"\n",
    "        \n",
    "        else:\n",
    "            answer_form = \"word\"\n",
    "            answer_ner = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821d8e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daratan utama Skotlandia mencakup sepertiga da...</td>\n",
       "      <td>Berapa luas Skotlandia pada tahun 1835?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Belgia (secara resmi disebut Kerajaan Belgia) ...</td>\n",
       "      <td>Berapa luas negara Polandia?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Never Let Me Go (2005) adalah sebuah novel fik...</td>\n",
       "      <td>Kapan novel What Remains of the Year dirilis?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sejarah film tidak bisa lepas dari sejarah fot...</td>\n",
       "      <td>Siapakah yang memasarkan Kamera pertama kali ?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Salah satu strategi yang diusulkan untuk menan...</td>\n",
       "      <td>Apakah pendidikan tertinggi BUY Bakae Hirose?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kode Etik Jurnalistik adalah himpunan etika pr...</td>\n",
       "      <td>Apa ketentuan hukum Jurnalistik di Inggris?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Deutsche Mark (DM, DEM) atau Mark Jerman adala...</td>\n",
       "      <td>Berapa banyak mata uang negara Jerman?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Taylor Swift adalah album studio debut eponymo...</td>\n",
       "      <td>Apa nama album pertama Taylor Taylor?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Djoko mengikuti sekolah di sekolah umum karena...</td>\n",
       "      <td>Di manakah orang tua Djoko Suryo?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hold On to Sixteen adalah episode kedelapan mu...</td>\n",
       "      <td>Kapan Radar Overstreet pertama kali ditayangkan?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context  \\\n",
       "1   Daratan utama Skotlandia mencakup sepertiga da...   \n",
       "8   Belgia (secara resmi disebut Kerajaan Belgia) ...   \n",
       "9   Never Let Me Go (2005) adalah sebuah novel fik...   \n",
       "11  Sejarah film tidak bisa lepas dari sejarah fot...   \n",
       "12  Salah satu strategi yang diusulkan untuk menan...   \n",
       "13  Kode Etik Jurnalistik adalah himpunan etika pr...   \n",
       "15  Deutsche Mark (DM, DEM) atau Mark Jerman adala...   \n",
       "16  Taylor Swift adalah album studio debut eponymo...   \n",
       "17  Djoko mengikuti sekolah di sekolah umum karena...   \n",
       "18  Hold On to Sixteen adalah episode kedelapan mu...   \n",
       "\n",
       "                                            question answer  \n",
       "1            Berapa luas Skotlandia pada tahun 1835?         \n",
       "8                       Berapa luas negara Polandia?         \n",
       "9      Kapan novel What Remains of the Year dirilis?         \n",
       "11    Siapakah yang memasarkan Kamera pertama kali ?         \n",
       "12     Apakah pendidikan tertinggi BUY Bakae Hirose?         \n",
       "13       Apa ketentuan hukum Jurnalistik di Inggris?         \n",
       "15            Berapa banyak mata uang negara Jerman?         \n",
       "16             Apa nama album pertama Taylor Taylor?         \n",
       "17                 Di manakah orang tua Djoko Suryo?         \n",
       "18  Kapan Radar Overstreet pertama kali ditayangkan?         "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811b1295",
   "metadata": {},
   "source": [
    "## Delete all unanswerable row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qas_train_df = data_qas_train_df[data_qas_train_df['answer'] != '']\n",
    "data_qas_val_df = data_qas_val_df[data_qas_val_df['answer'] != '']\n",
    "data_qas_test_df = data_qas_test_df[data_qas_test_df['answer'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3c125",
   "metadata": {},
   "source": [
    "### Reset index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qas_train_df = data_qas_train_df.reset_index(drop=True)\n",
    "data_qas_val_df = data_qas_val_df.reset_index(drop=True)\n",
    "data_qas_test_df = data_qas_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881d292",
   "metadata": {},
   "source": [
    "## Create NLI dataset from copy of QA dataset above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df = data_qas_train_df.copy()\n",
    "data_nli_val_df = data_qas_val_df.copy()\n",
    "data_nli_test_df = data_qas_test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83072dc4",
   "metadata": {},
   "source": [
    "## Convert context pair to premise (only renaming column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1562622",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df = data_nli_train_df.rename(columns={\"context\": \"premise\"})\n",
    "data_nli_val_df = data_nli_val_df.rename(columns={\"context\": \"premise\"})\n",
    "data_nli_test_df = data_nli_test_df.rename(columns={\"context\": \"premise\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33986ce",
   "metadata": {},
   "source": [
    "# Add contradiction label cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095664cc",
   "metadata": {},
   "source": [
    "## Import pipeline to create contradiction cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8850d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tools_ner = pipeline(task = TOOLS_NAME_NER, \n",
    "                     model = MODEL_TOOLS_NAME_NER, \n",
    "                     tokenizer = AutoTokenizer.from_pretrained(MODEL_TOOLS_NAME_NER, \n",
    "                                                               model_max_length=512, \n",
    "                                                               truncation=True),\n",
    "                     aggregation_strategy = 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e84dda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tools_chunking = pipeline(task = TOOLS_NAME_POS, \n",
    "                     model = MODEL_TOOLS_NAME_POS, \n",
    "                     tokenizer = AutoTokenizer.from_pretrained(MODEL_TOOLS_NAME_POS, \n",
    "                                                               model_max_length=512, \n",
    "                                                               truncation=True),\n",
    "                     aggregation_strategy = 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae4a76",
   "metadata": {},
   "source": [
    "## Add NER and chunking tag column in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c888804-88ec-4858-ba18-131545ee6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_tag(answer, tag, ner=nlp_tools_ner, chunking=nlp_tools_chunking):\n",
    "\n",
    "    if tag == \"ner\": tools=ner\n",
    "    else: tools=chunking\n",
    "\n",
    "    retrieved_from_tools = tools(answer)\n",
    "    tag_answer_list = []\n",
    "    \n",
    "    if len(retrieved_from_tools) != 0:\n",
    "        for i in retrieved_from_tools:\n",
    "            tag_answer = (i['entity_group'], i['word'])\n",
    "            tag_answer_list.append(tag_answer)\n",
    "    else:\n",
    "        tag_answer = (\"NULL\", answer)\n",
    "        tag_answer_list.append(tag_answer)\n",
    "        \n",
    "    return tag_answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee495118-61e1-4603-9194-690c0ae737c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_premise_tag(data, tag, index, premise_array, ner=nlp_tools_ner, chunking=nlp_tools_chunking):\n",
    "\n",
    "    if tag == \"ner\": tools=ner\n",
    "    else: tools=chunking\n",
    "    \n",
    "    if len(tools(data['premise'][index])) == 0:\n",
    "        premise_array.append(\"NO TOKEN DETECTED\")\n",
    "    \n",
    "    else:\n",
    "        for j in tools(data['premise'][index]):\n",
    "            tag_premise = (j['entity_group'], j['word'])\n",
    "            premise_array.append(tag_premise)\n",
    "\n",
    "    return premise_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ner_and_chunking_all_tag(data):\n",
    "    \n",
    "    data['ner_tag_answer'] = \"\"\n",
    "    data['chunking_tag_answer'] = \"\"\n",
    "    \n",
    "    data['ner_tag_premise'] = \"\"\n",
    "    data['chunking_tag_premise'] = \"\"\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        \n",
    "        answer = data['answer'][i]\n",
    "        premise = data['premise'][i]\n",
    "        \n",
    "        ner_premise_array = []\n",
    "        chunking_premise_array = []\n",
    "            \n",
    "        data['ner_tag_answer'][i] = add_row_tag(answer, \"ner\")\n",
    "        data['chunking_tag_answer'][i] = add_row_tag(answer, \"chunking\")\n",
    "                                                \n",
    "        data['ner_tag_premise'][i] = add_premise_tag(data, \"ner\", i, ner_premise_array)\n",
    "        data['chunking_tag_premise'][i] = add_premise_tag(data, \"chunking\", i, chunking_premise_array)  \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cad8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df = add_ner_and_chunking_all_tag(data_nli_train_df)\n",
    "data_nli_val_df = add_ner_and_chunking_all_tag(data_nli_val_df)\n",
    "data_nli_test_df = add_ner_and_chunking_all_tag(data_nli_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49a3e4",
   "metadata": {},
   "source": [
    "# Create wrong answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b5991",
   "metadata": {},
   "source": [
    "This is the flow to create wrong answer:\n",
    "\n",
    "1. Check the NER and POS/Chunking labels of the right_answer and context/premise.\n",
    "\n",
    "2. Search and group NER and POS/Chunking labels that match the right_answer throughout the context/premise.\n",
    "\n",
    "3. Perform NER classification. There will be two branches here, namely:\n",
    "\n",
    "   3a. If the NER of the right_answer can be detected, then calculate the distance using semantic similarity or word vectors between the right_answer and various possible wrong_answers with the same NER as the right_answer. Once done, proceed to the final wrong_answer.\n",
    "   \n",
    "   3b. If the NER of the right_answer cannot be detected (NULL) or context/premise does not contain any of NER of right_answer, then the POS/Chunking of the right_answer will be identified.\n",
    "   \n",
    "4. Perform POS/Chunking classification. Continuation from point 3b. There will be two more branches:\n",
    "\n",
    "   4a. If the POS/Chunking of the right_answer can be detected, then calculate the distance using semantic similarity or word vectors between the right_answer and various possible wrong_answers with the same POS/Chunking as the right_answer. Once done, proceed to the final wrong_answer.\n",
    "   \n",
    "   4b. If the POS/Chunking of the right_answer cannot be detected (NULL) or context/premise does not contain any of NER of right_answer, then the final wrong_answer will be chosen based on a random word (random_word) from the context/premise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d00a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_similarity = SentenceTransformer(MODEL_SIMILARITY_NAME)\n",
    "\n",
    "def return_similarity_sorted_array(right_answer, sentence_array, model=model_similarity):\n",
    "    \n",
    "    embedding_right_answer = model.encode([right_answer], convert_to_tensor=True)\n",
    "    embedding_sentence_array = model.encode(sentence_array, convert_to_tensor=True)\n",
    "    \n",
    "    cosine_scores = util.pytorch_cos_sim(embedding_right_answer, embedding_sentence_array)\n",
    "    \n",
    "    sorted_indices = cosine_scores.argsort(descending=True)[0]\n",
    "    sorted_array = [sentence_array[i] for i in sorted_indices]\n",
    "    \n",
    "    return sorted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_values_with_hash(arr):\n",
    "    return [item for item in arr if \"#\" not in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_word(text):\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    random_word = random.choice(words)\n",
    "    return random_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0afd1-e751-4cb8-ae22-1b7bb10a5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_same_tag(tag_answers, tag_premise, same_tag_array):\n",
    "    \n",
    "    for tag in tag_premise:\n",
    "\n",
    "        # Check is it in tuple?\n",
    "        if isinstance(tag, tuple):\n",
    "            tag_word = tag[0]\n",
    "        else:\n",
    "            tag_word = None\n",
    "\n",
    "        for tag_answer in tag_answers:\n",
    "            if tag_answer == tag_word:\n",
    "                same_tag_array.append(tag[1])\n",
    "\n",
    "    return remove_values_with_hash(same_tag_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369eced-0cee-4d0d-a436-89a97d2af242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_similarity(data, right_answer, index, tag, plausible_answer_array):\n",
    "\n",
    "    if tag == \"ner\": slice='same_ner_tag_answer'\n",
    "    elif tag == \"chunking\": slice='same_chunking_tag_answer'\n",
    "    else: slice=None\n",
    "\n",
    "    # Find all the sorted (by similarity) plausible wrong answer, \n",
    "    # and remove hask & punctuation only answer\n",
    "    if slice != None:\n",
    "        wrong_answer_array = return_similarity_sorted_array(right_answer, data[slice][index])\n",
    "    else:\n",
    "        wrong_answer_array = return_similarity_sorted_array(right_answer, plausible_answer_array)\n",
    "    \n",
    "    plausible_answer_array = remove_values_with_hash(wrong_answer_array)\n",
    "    plausible_answer_array = [string for string in plausible_answer_array \\\n",
    "                                      if not contains_only_punctuation(string)]\n",
    "\n",
    "    # Only return the most similar to right_answer\n",
    "    wrong_answer = plausible_answer_array[0]\n",
    "    \n",
    "    assert isinstance(wrong_answer, str)\n",
    "    assert isinstance(plausible_answer_array, list)\n",
    "    \n",
    "    if tag == \"ner\": \n",
    "        properties = \"\"\"IDENTICAL NER labels were found, and the highest similarity \n",
    "                                    score same NER array was selected\"\"\"\n",
    "    elif tag == \"chunking\":\n",
    "        properties = \"\"\"IDENTICAL Chunking labels were found, and the highest similarity \n",
    "                                        score from same Chunking array was selected\"\"\"\n",
    "    else:\n",
    "        properties = \"\"\"NO CHUNKING labels were found, and the highest similarity score \n",
    "                                        from plausible answer was selected\"\"\"\n",
    "    \n",
    "    return wrong_answer, plausible_answer_array, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e980a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_substring_span(long_string, substring):\n",
    "    long_string = long_string.lower()\n",
    "    substring = substring.lower()\n",
    "    \n",
    "    start_index = long_string.find(substring)\n",
    "    \n",
    "    if start_index != -1:\n",
    "        end_index = start_index + len(substring) - 1\n",
    "        return start_index, end_index\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6eef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_span_overlap(span1, span2):\n",
    "    if span1 == None or span2 == None: return True # Exit plan\n",
    "    else: return span1[0] <= span2[1] and span2[0] <= span1[1]\n",
    "\n",
    "def check_string_overlap(str1, str2):\n",
    "    assert isinstance(str1, str)\n",
    "    assert isinstance(str1, str)\n",
    "    \n",
    "    return (str1[-1] >= str2[0]) \\\n",
    "            or (str1 in str2) \\\n",
    "            or (str2 in str1)\n",
    "\n",
    "def contains_only_punctuation(text):\n",
    "    return all(char in string.punctuation for char in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fc59b-cb40-4dc3-a324-e850b1324a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_same_answer(right_answer, \n",
    "                        wrong_answer, \n",
    "                        premise, \n",
    "                        plausible_answer_array):\n",
    "    \n",
    "    # Removing right answer & wrong answer in this particular time\n",
    "    plausible_answer_array = [item for item in plausible_answer_array \\\n",
    "                              if item not in [right_answer, wrong_answer]]\n",
    "\n",
    "    if len(plausible_answer_array) <= 1:\n",
    "        wrong_answer = select_random_word(premise)\n",
    "        properties = \"\"\"Detected span that is the SAME as the right answer, \n",
    "                                search random word from premise\"\"\"\n",
    "\n",
    "    else:\n",
    "        wrong_answer = plausible_answer_array[0] # Take the highest value in the sorted array\n",
    "        properties = \"\"\"Detected span that is the SAME as the right answer, \n",
    "                                search the highest value in the sorted array\"\"\"\n",
    "\n",
    "    return wrong_answer, properties, plausible_answer_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_multiple_label(data, index, tag):\n",
    "    \n",
    "    if tag == \"ner\": slice='ner_tag_answer'\n",
    "    elif tag == \"chunking\": slice='chunking_tag_answer'\n",
    "    else: pass\n",
    "        \n",
    "    if len(data[slice][index]) > 1: return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0097c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer_match_to_multiple_label(data, \n",
    "                                          index, \n",
    "                                          tag,\n",
    "                                          wrong_answer, \n",
    "                                          plausible_answer_array):\n",
    "    \n",
    "    if tag == \"none\":\n",
    "        for answer in plausible_answer_array:\n",
    "            if len(answer.split()) == len(wrong_answer.split()):\n",
    "                wrong_answer = answer\n",
    "                break\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if is_multiple_label(data, index, tag):\n",
    "            # Check if wrong_answer a form of sentence?\n",
    "            # If not, look for a wrong_answer in the form of a sentence\n",
    "            if len(wrong_answer.split()) == 1:\n",
    "                for answer in plausible_answer_array:\n",
    "                    if len(answer.split()) > 1:\n",
    "                        wrong_answer = answer\n",
    "                        break\n",
    "\n",
    "        else:\n",
    "            # Check if wrong_answer a form of word?\n",
    "            # If not, look for a wrong_answer in the form of a word\n",
    "            if len(wrong_answer.split()) > 1:\n",
    "                for answer in plausible_answer_array:\n",
    "                    if len(answer.split()) == 1:\n",
    "                        wrong_answer = answer\n",
    "                        break\n",
    "                        \n",
    "    # We can try this out, actually.\n",
    "    # With this code, we only have just check the length of each answer. \n",
    "    # We don't need is_multiple_label function check.\n",
    "    #if len(right_answer) != len(wrong_answer):\n",
    "    #    for answer in plausible_answer_array:\n",
    "    #        if len(answer) > len(right_answer):\n",
    "    #            wrong_answer = answer\n",
    "    #            break\n",
    "                    \n",
    "    if tag == \"ner\": \n",
    "        properties = \"\"\"IDENTICAL NER labels were found, however, \n",
    "                        the final wrong_answer is sought \n",
    "                        which is a form of a sentence\"\"\"\n",
    "    \n",
    "    elif tag == \"chunking\":\n",
    "        properties = \"\"\"IDENTICAL Chunking labels were found, however, \n",
    "                        the final wrong_answer is sought \n",
    "                        which is a form of a word\"\"\"\n",
    "    \n",
    "    elif tag == \"none\":\n",
    "        properties = \"\"\"NO CHUNKING labels were found, however, \n",
    "                        the final wrong_answer is sought\n",
    "                        which is a same form (word or sentence)\"\"\"\n",
    "                    \n",
    "    return wrong_answer, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97759144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wrong_answer(data):\n",
    "    \n",
    "    data['same_ner_tag_answer'] = \"\"\n",
    "    data['same_chunking_tag_answer'] = \"\"\n",
    "    data['wrong_answer'] = \"\"\n",
    "    data['plausible_answer_based_on_method'] = \"\"\n",
    "    data['properties'] = \"\"\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        \n",
    "        right_answer = data['answer'][i]\n",
    "        premise = data['premise'][i]\n",
    "\n",
    "        same_ner_tag_answer_array = []\n",
    "        same_chunking_tag_answer_array = []\n",
    "\n",
    "        ner_tag_answer = data['ner_tag_answer'][i][0]\n",
    "        ner_tag_premise = data['ner_tag_premise'][i]\n",
    "\n",
    "        chunking_tag_answer = data['chunking_tag_answer'][i][0]\n",
    "        chunking_tag_premise = data['chunking_tag_premise'][i]\n",
    "        \n",
    "        # Grouped with the same NER & Chunking group, between answer and word of premise\n",
    "        data['same_ner_tag_answer'][i] = grouping_same_tag(ner_tag_answer,\n",
    "                                                           ner_tag_premise,\n",
    "                                                           same_ner_tag_answer_array)\n",
    "        \n",
    "        data['same_chunking_tag_answer'][i] = grouping_same_tag(chunking_tag_answer, \n",
    "                                                                chunking_tag_premise, \n",
    "                                                                same_chunking_tag_answer_array)\n",
    "               \n",
    "        # Start to create wrong answer\n",
    "        plausible_answer_array = []\n",
    "\n",
    "        # Perform NER classification\n",
    "        # If the NER of the right_answer can be detected, then calculate the distance using semantic \n",
    "        # similarity or word vectors between the right_answer and various possible wrong_answers with \n",
    "        # the same NER as the right_answer. Once done, proceed to the final wrong_answer.\n",
    "        if data['same_ner_tag_answer'][i] != []:\n",
    "            wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                      i, \"ner\", plausible_answer_array)\n",
    "            wrong_answer, properties = create_answer_match_to_multiple_label(data, i, \"ner\", wrong_answer,\n",
    "                                                                             plausible_answer_array)\n",
    "            \n",
    "        # If the NER of the right_answer cannot be detected (NULL) or context/premise does not contain \n",
    "        # any of NER of right_answer, then the POS/Chunking of the right_answer will be identified.\n",
    "        # Perform POS/Chunking classification\n",
    "        else:\n",
    "            \n",
    "            # If the POS/Chunking of the right_answer can be detected, then calculate the distance \n",
    "            # using semantic similarity or word vectors between the right_answer and various possible \n",
    "            # wrong_answers with the same POS/Chunking as the right_answer. Once done, proceed to the \n",
    "            # final wrong_answer.\n",
    "            if data['same_chunking_tag_answer'][i] != []:\n",
    "                wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                          i, \"chunking\", plausible_answer_array)\n",
    "                wrong_answer, properties = create_answer_match_to_multiple_label(data, i, \"chunking\", wrong_answer,\n",
    "                                                                             plausible_answer_array)\n",
    "            \n",
    "            # If the POS/Chunking of the right_answer cannot be detected (NULL) or context/premise \n",
    "            # does not contain any of NER of right_answer, then the final wrong_answer will be chosen \n",
    "            # based on a random word (random_word) from the context/premise.\n",
    "            else:\n",
    "                for chunking_tag in chunking_tag_premise:\n",
    "                    plausible_answer_array.append(chunking_tag[1])\n",
    "\n",
    "                wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                          i, \"none\", plausible_answer_array)\n",
    "                wrong_answer, properties = create_answer_match_to_multiple_label(data, i, \"none\", wrong_answer,\n",
    "                                                                             plausible_answer_array)\n",
    "\n",
    "        # Check for preventing same answer for right_answer and wrong_answer  \n",
    "        right_answer_span = find_substring_span(premise, right_answer)\n",
    "        wrong_answer_span = find_substring_span(premise, wrong_answer)\n",
    "        \n",
    "        is_span_or_same_literal = check_span_overlap(right_answer_span, wrong_answer_span) \\\n",
    "                or check_string_overlap(right_answer.lower(), wrong_answer.lower())\n",
    "\n",
    "        if is_span_or_same_literal:\n",
    "\n",
    "            # Removing right answer & wrong answer in this particular time\n",
    "            wrong_answer, properties, plausible_answer_array = replace_same_answer(right_answer, \n",
    "                                                                                  wrong_answer, \n",
    "                                                                                  premise, \n",
    "                                                                                  plausible_answer_array)\n",
    "            data['properties'][i] = properties\n",
    "        \n",
    "        data['wrong_answer'][i] = wrong_answer\n",
    "        data['plausible_answer_based_on_method'][i] = plausible_answer_array\n",
    "            \n",
    "    return data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5d11b-872d-4620-98a9-74a37b5248f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wrong_answer_with_removing_invalid_data(data):\n",
    "    \n",
    "    data['same_ner_tag_answer'] = \"\"\n",
    "    data['same_chunking_tag_answer'] = \"\"\n",
    "    data['wrong_answer'] = \"\"\n",
    "    data['plausible_answer_based_on_method'] = \"\"\n",
    "    data['properties'] = \"\"\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        \n",
    "        right_answer = data['answer'][i]\n",
    "        premise = data['premise'][i]\n",
    "\n",
    "        same_ner_tag_answer_array = []\n",
    "        same_chunking_tag_answer_array = []\n",
    "\n",
    "        ner_tag_answer = data['ner_tag_answer'][i][0]\n",
    "        ner_tag_premise = data['ner_tag_premise'][i]\n",
    "\n",
    "        chunking_tag_answer = data['chunking_tag_answer'][i][0]\n",
    "        chunking_tag_premise = data['chunking_tag_premise'][i]\n",
    "        \n",
    "        # Grouped with the same NER & Chunking group, between answer and word of premise\n",
    "        data['same_ner_tag_answer'][i] = grouping_same_tag(ner_tag_answer,\n",
    "                                                           ner_tag_premise,\n",
    "                                                           same_ner_tag_answer_array)\n",
    "        \n",
    "        data['same_chunking_tag_answer'][i] = grouping_same_tag(chunking_tag_answer, \n",
    "                                                                chunking_tag_premise, \n",
    "                                                                same_chunking_tag_answer_array)\n",
    "               \n",
    "        # Start to create wrong answer\n",
    "        plausible_answer_array = []\n",
    "\n",
    "        # Perform NER classification\n",
    "        # If the NER of the right_answer can be detected, then calculate the distance using semantic \n",
    "        # similarity or word vectors between the right_answer and various possible wrong_answers with \n",
    "        # the same NER as the right_answer. Once done, proceed to the final wrong_answer.\n",
    "        if data['same_ner_tag_answer'][i] != []:\n",
    "            wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                      i, \"ner\", plausible_answer_array)\n",
    "            wrong_answer, properties = create_answer_match_to_multiple_label(data, i, \"ner\", wrong_answer,\n",
    "                                                                             plausible_answer_array)\n",
    "            \n",
    "        # If the NER of the right_answer cannot be detected (NULL) or context/premise does not contain \n",
    "        # any of NER of right_answer, then drop that particular row data.\n",
    "        else:\n",
    "            data.drop(i, inplace=True)\n",
    "            data.reset_index(drop=True)\n",
    "            continue\n",
    "        \n",
    "        # Check for preventing same answer for right_answer and wrong_answer  \n",
    "        right_answer_span = find_substring_span(premise, right_answer)\n",
    "        wrong_answer_span = find_substring_span(premise, wrong_answer)\n",
    "        \n",
    "        is_span_or_same_literal = check_span_overlap(right_answer_span, wrong_answer_span) \\\n",
    "                or check_string_overlap(right_answer.lower(), wrong_answer.lower())\n",
    "\n",
    "        if is_span_or_same_literal:\n",
    "\n",
    "            # I'm still confused, whether the overlapping \n",
    "            # answers (either in span or its literal form) \n",
    "            # should also be dropped or not.\n",
    "            # If it's dropped, then, uncomment 3 lines of\n",
    "            # code below.\n",
    "            \n",
    "            #data.drop(i, inplace=True)\n",
    "            #data.reset_index(drop=True)\n",
    "            #continue\n",
    "\n",
    "            # Removing right answer & wrong answer in this particular time\n",
    "            wrong_answer, properties, plausible_answer_array = replace_same_answer(right_answer, \n",
    "                                                                                  wrong_answer, \n",
    "                                                                                  premise, \n",
    "                                                                                  plausible_answer_array)\n",
    "            data['properties'][i] = properties\n",
    "        \n",
    "        data['wrong_answer'][i] = wrong_answer\n",
    "        data['plausible_answer_based_on_method'][i] = plausible_answer_array\n",
    "            \n",
    "    return data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be56d2-8924-47aa-9009-b7378190e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = create_wrong_answer_with_removing_invalid_data(data_nli_train_df)\n",
    "#y = create_wrong_answer_with_removing_invalid_data(data_nli_val_df)\n",
    "#z = create_wrong_answer_with_removing_invalid_data(data_nli_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df = create_wrong_answer(data_nli_train_df)\n",
    "data_nli_val_df = create_wrong_answer(data_nli_val_df)\n",
    "data_nli_test_df = create_wrong_answer(data_nli_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c56dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe61c82",
   "metadata": {},
   "source": [
    "# Split to two dataset: right dataset & wrong dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_column_number(data, column_name=\"hypothesis\", column_num=3):\n",
    "\n",
    "    cols = list(data.columns)\n",
    "    cols.remove(column_name)\n",
    "    cols.insert(column_num, column_name)\n",
    "\n",
    "    data = data[cols]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56880d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['wrong_answer']\n",
    "\n",
    "data_nli_right_train_df = data_nli_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_right_val_df = data_nli_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_right_test_df = data_nli_test_df.drop(columns=columns_to_exclude).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['answer']\n",
    "\n",
    "data_nli_wrong_train_df = data_nli_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_wrong_val_df = data_nli_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_wrong_test_df = data_nli_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_wrong_train_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "data_nli_wrong_val_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "data_nli_wrong_test_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "\n",
    "data_nli_wrong_train_df = move_to_column_number(data_nli_wrong_train_df, \"answer\", 2)\n",
    "data_nli_wrong_val_df = move_to_column_number(data_nli_wrong_val_df, \"answer\", 2)\n",
    "data_nli_wrong_test_df = move_to_column_number(data_nli_wrong_test_df, \"answer\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e6f08",
   "metadata": {},
   "source": [
    "# Convert question-answer pair to hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_question_and_answer_to_hypothesis(data):\n",
    "    for i in range(len(data)):\n",
    "        data['hypothesis'] = data['question'] + ' ' + data['answer']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf340e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_right_train_df = convert_question_and_answer_to_hypothesis(data_nli_right_train_df)\n",
    "data_nli_right_val_df = convert_question_and_answer_to_hypothesis(data_nli_right_val_df)\n",
    "data_nli_right_test_df = convert_question_and_answer_to_hypothesis(data_nli_right_test_df)\n",
    "\n",
    "data_nli_right_train_df = move_to_column_number(data_nli_right_train_df, \"hypothesis\", 3)\n",
    "data_nli_right_val_df = move_to_column_number(data_nli_right_val_df, \"hypothesis\", 3)\n",
    "data_nli_right_test_df = move_to_column_number(data_nli_right_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20241aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_wrong_train_df = convert_question_and_answer_to_hypothesis(data_nli_wrong_train_df)\n",
    "data_nli_wrong_val_df = convert_question_and_answer_to_hypothesis(data_nli_wrong_val_df)\n",
    "data_nli_wrong_test_df = convert_question_and_answer_to_hypothesis(data_nli_wrong_test_df)\n",
    "\n",
    "data_nli_wrong_train_df = move_to_column_number(data_nli_wrong_train_df, \"hypothesis\", 3)\n",
    "data_nli_wrong_val_df = move_to_column_number(data_nli_wrong_val_df, \"hypothesis\", 3)\n",
    "data_nli_wrong_test_df = move_to_column_number(data_nli_wrong_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c4ace",
   "metadata": {},
   "source": [
    "# Add label: entailment & contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_right_train_df['label'] = 'entailment'\n",
    "data_nli_right_val_df['label'] = 'entailment'\n",
    "data_nli_right_test_df['label'] = 'entailment'\n",
    "\n",
    "data_nli_right_train_df = move_to_column_number(data_nli_right_train_df, \"label\", 4)\n",
    "data_nli_right_train_df = move_to_column_number(data_nli_right_val_df, \"label\", 4)\n",
    "data_nli_right_train_df = move_to_column_number(data_nli_right_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02098578",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_wrong_train_df['label'] = 'contradiction'\n",
    "data_nli_wrong_val_df['label'] = 'contradiction'\n",
    "data_nli_wrong_test_df['label'] = 'contradiction'\n",
    "\n",
    "data_nli_wrong_train_df = move_to_column_number(data_nli_wrong_train_df, \"label\", 4)\n",
    "data_nli_wrong_val_df = move_to_column_number(data_nli_wrong_val_df, \"label\", 4)\n",
    "data_nli_wrong_test_df = move_to_column_number(data_nli_wrong_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f431144",
   "metadata": {},
   "source": [
    "# Concat the right and wrong NLI to one NLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df_final = pd.concat([data_nli_right_train_df, data_nli_wrong_train_df], axis=0, ignore_index=True)\n",
    "data_nli_val_df_final = pd.concat([data_nli_right_val_df, data_nli_wrong_val_df], axis=0, ignore_index=True)\n",
    "data_nli_test_df_final = pd.concat([data_nli_right_test_df, data_nli_wrong_test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c82400",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea9097",
   "metadata": {},
   "source": [
    "# Convert to DataFrame format to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df_final.to_csv(\"data_nli_train_df.csv\", index=False)\n",
    "data_nli_val_df_final.to_csv(\"data_nli_val_df.csv\", index=False)\n",
    "data_nli_test_df_final.to_csv(\"data_nli_test_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28862303-7b4c-4279-91b3-b5efcdf00e75",
   "metadata": {},
   "source": [
    "# Push to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d50f6c-1e40-4087-9f6c-f5bc5aaaa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUB_TOKEN = \"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\"\n",
    "USER = \"muhammadravi251001\"\n",
    "REPO = \"idk-mrc-nli\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"data_nli_train_df.csv\",\n",
    "    path_in_repo=\"data_nli_train_df.csv\",\n",
    "    repo_id=f\"{USER}/{REPO}\",\n",
    "    token=HUB_TOKEN,\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"data_nli_val_df.csv\",\n",
    "    path_in_repo=\"data_nli_val_df.csv\",\n",
    "    repo_id=f\"{USER}/{REPO}\",\n",
    "    token=HUB_TOKEN,\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"data_nli_test_df.csv\",\n",
    "    path_in_repo=\"data_nli_test_df.csv\",\n",
    "    repo_id=f\"{USER}/{REPO}\",\n",
    "    token=HUB_TOKEN,\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
