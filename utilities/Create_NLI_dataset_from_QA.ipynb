{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9171daa4",
   "metadata": {},
   "source": [
    "# Define tool and model of the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a063a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 27 12:40:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   58C    P0   262W / 300W |  31956MiB / 32510MiB |     95%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   50C    P0   121W / 300W |  31842MiB / 32510MiB |     93%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   48C    P0   105W / 300W |  30374MiB / 32510MiB |     27%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   45C    P0   109W / 300W |  29256MiB / 32510MiB |     78%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    60W / 300W |    962MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    40W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cef3dd",
   "metadata": {},
   "source": [
    "Below, it is some settings to run in my local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59d867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38400c",
   "metadata": {},
   "source": [
    "You can tweak your settings too in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41849a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "NAME = \"idk-mrc\"\n",
    "NO_ANSWER_STATEMENT = \"Tidak ada jawaban\"\n",
    "\n",
    "TASK_NER_NAME = \"ner\"\n",
    "MODEL_NER_NAME = \"ageng-anugrah/indobert-large-p2-finetuned-ner\"\n",
    "\n",
    "TASK_CHUNKING_NAME = \"token-classification\"\n",
    "MODEL_CHUNKING_NAME = \"ageng-anugrah/indobert-large-p2-finetuned-chunking\"\n",
    "\n",
    "MODEL_SIMILARITY_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\n",
    "URL_STOPWORD = \"https://raw.githubusercontent.com/6/stopwords-json/master/stopwords-all.json\"\n",
    "\n",
    "TASK_PARAPHRASER_NAME = \"text2text-generation\"\n",
    "MODEL_PARAPHRASER_NAME = \"\"\n",
    "\n",
    "# Uncomment sys.maxsize to create all of the data, \n",
    "# else if you want to debugging\n",
    "\n",
    "# SAMPLE = sys.maxsize\n",
    "SAMPLE = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9fdd4",
   "metadata": {},
   "source": [
    "# Import anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26d313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "import string\n",
    "import contextlib\n",
    "import gc\n",
    "import random\n",
    "import string\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback, \n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079cd27c",
   "metadata": {},
   "source": [
    "# Retrieve QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f426d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM STARTED\n"
     ]
    }
   ],
   "source": [
    "print(\"PROGRAM STARTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dafbf0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset idk_mrc (/root/.cache/huggingface/datasets/idk_mrc/idk_mrc_source/1.0.0/cf468d86fa7341e69998db1449851672ebfb4fa46036929d66b9de15c421334f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4345735a9845c2bd54f5e739704e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3659/3659 [00:16<00:00, 226.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 358/358 [00:01<00:00, 280.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 378/378 [00:01<00:00, 271.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 9332\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 764\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas = conhelps.filtered(lambda x: 'idk_mrc' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas['train'])\n",
    "df_validation = pd.DataFrame(data_qas['validation'])\n",
    "df_test = pd.DataFrame(data_qas['test'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_train['context']))):\n",
    "    for j in df_train[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                           \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                           \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                           ignore_index=True)\n",
    "        else:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": str(), \n",
    "                                                           \"answer_start\": 0, \n",
    "                                                           \"answer_end\": 0}}, \n",
    "                                                           ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_validation['context']))):\n",
    "    for j in df_validation[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)        \n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_test = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_test['context']))):\n",
    "    for j in df_test[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "test_dataset = Dataset.from_dict(new_df_test)\n",
    "\n",
    "data_qas = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})\n",
    "data_qas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae908a",
   "metadata": {},
   "source": [
    "# Convert to NLI, with hypothesis being just do concat question & answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c79ac",
   "metadata": {},
   "source": [
    "## Convert Dataset to DataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b342b8ce-41f9-4714-84a5-1697cfee1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42, the answer to life the universe and everything\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275dc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to training all of the data (prod),\n",
    "# this code will convert to DataFrame.\n",
    "# However, if you want to debug (not-prod),\n",
    "# this code will convert SAMPLE of your DataFrame\n",
    "\n",
    "if SAMPLE == sys.maxsize:\n",
    "    data_qas_train_df = pd.DataFrame(data_qas[\"train\"][:SAMPLE])\n",
    "    data_qas_val_df = pd.DataFrame(data_qas[\"validation\"][:SAMPLE])\n",
    "    data_qas_test_df = pd.DataFrame(data_qas[\"test\"][:SAMPLE])\n",
    "\n",
    "else:\n",
    "    data_qas_train_df = (pd.DataFrame(data_qas[\"train\"])).sample(n=SAMPLE, random_state=42)\n",
    "    data_qas_val_df = (pd.DataFrame(data_qas[\"validation\"])).sample(n=SAMPLE, random_state=42)\n",
    "    data_qas_test_df = (pd.DataFrame(data_qas[\"test\"])).sample(n=SAMPLE, random_state=42)\n",
    "\n",
    "    data_qas_train_df = data_qas_train_df.reset_index(drop=True)\n",
    "    data_qas_val_df = data_qas_val_df.reset_index(drop=True)\n",
    "    data_qas_test_df = data_qas_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bbf0b",
   "metadata": {},
   "source": [
    "## Retrieve answer text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0424485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retrieve answer text\n",
    "# Because, we do not use answer_start\n",
    "# and answer_end\n",
    "\n",
    "def retrieve_answer_text(data):\n",
    "    for i in range(len(data)):\n",
    "        data['answer'][i] = data['answer'][i]['text']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b1a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qas_train_df = retrieve_answer_text(data_qas_train_df)\n",
    "data_qas_val_df = retrieve_answer_text(data_qas_val_df)\n",
    "data_qas_test_df = retrieve_answer_text(data_qas_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881d292",
   "metadata": {},
   "source": [
    "## Create NLI dataset from copy of QA dataset above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8808af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df = data_qas_train_df.copy()\n",
    "data_nli_val_df = data_qas_val_df.copy()\n",
    "data_nli_test_df = data_qas_test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83072dc4",
   "metadata": {},
   "source": [
    "## Convert context pair to premise (only renaming column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1562622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming it, just for consistency\n",
    "\n",
    "data_nli_train_df = data_nli_train_df.rename(columns={\"context\": \"premise\"})\n",
    "data_nli_val_df = data_nli_val_df.rename(columns={\"context\": \"premise\"})\n",
    "data_nli_test_df = data_nli_test_df.rename(columns={\"context\": \"premise\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33986ce",
   "metadata": {},
   "source": [
    "# Add contradiction label cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095664cc",
   "metadata": {},
   "source": [
    "## Import pipeline to create contradiction cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8850d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tools_ner = pipeline(task = TASK_NER_NAME, \n",
    "                     model = MODEL_NER_NAME, \n",
    "                     tokenizer = AutoTokenizer.from_pretrained(MODEL_NER_NAME, \n",
    "                                                               model_max_length=512, \n",
    "                                                               truncation=True),\n",
    "                     aggregation_strategy = 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e84dda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tools_chunking = pipeline(task = TASK_CHUNKING_NAME, \n",
    "                     model = MODEL_CHUNKING_NAME, \n",
    "                     tokenizer = AutoTokenizer.from_pretrained(MODEL_CHUNKING_NAME, \n",
    "                                                               model_max_length=512, \n",
    "                                                               truncation=True),\n",
    "                     aggregation_strategy = 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae4a76",
   "metadata": {},
   "source": [
    "## Add NER and chunking tag column in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a68f3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code useful for cleaning the data (text)\n",
    "\n",
    "def remove_space_after_number_and_punctuation(text):\n",
    "    pattern = r'(\\d+)\\s*([.,])\\s*(?=\\S|$)'\n",
    "    cleaned_text = re.sub(pattern, r'\\1\\2', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee495118-61e1-4603-9194-690c0ae737c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code useful for tagging the entire premise\n",
    "# with NER and chunking tools\n",
    "\n",
    "def add_premise_tag(data, tag, index, premise_array, ner=nlp_tools_ner, chunking=nlp_tools_chunking):\n",
    "\n",
    "    if tag == \"ner\": tools=ner\n",
    "    else: tools=chunking\n",
    "    \n",
    "    # If the tools detected nothing, retrieve NO TOKEN DETECTED\n",
    "    if len(tools(data['premise'][index])) == 0:\n",
    "        premise_array.append(\"NO TOKEN DETECTED\")\n",
    "    \n",
    "    # Else if, the tools detected something, retrieve all of the entity and the word associated\n",
    "    else:\n",
    "        for j in tools(data['premise'][index]):\n",
    "            tag_premise = (j['entity_group'], remove_space_after_number_and_punctuation(j['word']))\n",
    "            premise_array.append(tag_premise)\n",
    "\n",
    "    return premise_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de4a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for clean the text off punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c888804-88ec-4858-ba18-131545ee6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code useful for tagging the entire answer\n",
    "# with NER and chunking tools\n",
    "\n",
    "def add_answer_tag(answer, tag, premise_array, ner=nlp_tools_ner, chunking=nlp_tools_chunking):\n",
    "\n",
    "    if tag == \"ner\": tools=ner\n",
    "    else: tools=chunking\n",
    "\n",
    "    tag_answer_list = list()\n",
    "    \n",
    "    if tag == \"ner\":\n",
    "        \n",
    "        # If tools in premise detecting some token\n",
    "        \n",
    "        if len(premise_array) != 0:\n",
    "            \n",
    "            for i in premise_array:\n",
    "                \n",
    "                # Extract the label and the word of premise\n",
    "                \n",
    "                label_from_premise_tag = i[0]\n",
    "                word_from_premise_tag = remove_space_after_number_and_punctuation(i[1])\n",
    "\n",
    "                # With assumption, that I do not dividing label when\n",
    "                # there is more than one label in one word answer.\n",
    "                # Instead, I give a NULL.\n",
    "\n",
    "                if word_from_premise_tag.lower() == answer.lower():\n",
    "                    tag_answer = (label_from_premise_tag, word_from_premise_tag)\n",
    "                    break\n",
    "\n",
    "                # Or, I could do this: to reducing NULL label with char not really with string.\n",
    "                \n",
    "                elif answer.lower() in word_from_premise_tag:\n",
    "                    tag_answer = (label_from_premise_tag, answer.lower())\n",
    "                    break\n",
    "                \n",
    "                # Then, if you still do not find the word, NULL given\n",
    "                \n",
    "                else:\n",
    "                    tag_answer = (\"NULL\", answer)\n",
    "            \n",
    "            tag_answer_list.append(tag_answer)\n",
    "\n",
    "        # If tools in premise NOT detecting some token, NULL given\n",
    "        \n",
    "        else:\n",
    "            tag_answer = (\"NULL\", answer)\n",
    "            tag_answer_list.append(tag_answer)\n",
    "    \n",
    "    elif tag == \"chunking\":\n",
    "        \n",
    "        # In chunking, it's slightly different because \n",
    "        # the basic assumption is that there are no NULL chunks,\n",
    "        # so it will capture all the chunk labels.\n",
    "        \n",
    "        retrieved_from_tools = tools(answer)\n",
    "\n",
    "        if len(retrieved_from_tools) != 0:\n",
    "            \n",
    "            for i in retrieved_from_tools:\n",
    "                tag_answer = (i['entity_group'], i['word'])\n",
    "                tag_answer_list.append(tag_answer)\n",
    "        \n",
    "        # So, it rarely going down there\n",
    "        # But, if really going down there\n",
    "        # from basic assumption, there is no NULL in chunking\n",
    "        # We can check subset of the sentence from premise,\n",
    "        # if found, we can take that particular label\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in premise_array:\n",
    "                \n",
    "                # Extract the label and the word of premise\n",
    "                \n",
    "                label_from_premise_tag = i[0]\n",
    "                word_from_premise_tag = remove_space_after_number_and_punctuation(i[1])\n",
    "                \n",
    "                # Take label from subset of sentence from premise\n",
    "                \n",
    "                if answer.lower() in word_from_premise_tag:\n",
    "                    tag_answer = (label_from_premise_tag, answer.lower())\n",
    "                    tag_answer_list.append(tag_answer)\n",
    "                    break\n",
    "            \n",
    "            # Use for and then direct else (for-else),\n",
    "            # if for-loop above not getting the break statement\n",
    "            \n",
    "            else:\n",
    "                tag_answer = (\"NULL\", answer)\n",
    "                tag_answer_list.append(tag_answer)\n",
    "        \n",
    "    return tag_answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4967bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper code to run\n",
    "# process for add tag to entire premise\n",
    "# and answer\n",
    "\n",
    "def add_ner_and_chunking_all_tag(data):\n",
    "    \n",
    "    data['ner_tag_answer'] = \"\"\n",
    "    data['chunking_tag_answer'] = \"\"\n",
    "    \n",
    "    data['ner_tag_premise'] = \"\"\n",
    "    data['chunking_tag_premise'] = \"\"\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        \n",
    "        answer = data['answer'][i]\n",
    "        premise = data['premise'][i]\n",
    "        \n",
    "        ner_premise_array = list()\n",
    "        chunking_premise_array = list()\n",
    "                                                \n",
    "        data['ner_tag_premise'][i] = add_premise_tag(data, \"ner\", i, ner_premise_array)\n",
    "        data['chunking_tag_premise'][i] = add_premise_tag(data, \"chunking\", i, chunking_premise_array)\n",
    "        \n",
    "        data['ner_tag_answer'][i] = add_answer_tag(answer, \"ner\", data['ner_tag_premise'][i])\n",
    "        data['chunking_tag_answer'][i] = add_answer_tag(answer, \"chunking\", data['chunking_tag_premise'][i])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25cad8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [01:37<00:00,  3.90s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [01:22<00:00,  3.30s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [01:05<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "data_nli_train_df = add_ner_and_chunking_all_tag(data_nli_train_df)\n",
    "data_nli_val_df = add_ner_and_chunking_all_tag(data_nli_val_df)\n",
    "data_nli_test_df = add_ner_and_chunking_all_tag(data_nli_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49a3e4",
   "metadata": {},
   "source": [
    "# Create wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0d00a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# This function useful for sorting the closest distance\n",
    "# by using embedding\n",
    "\n",
    "model_similarity = SentenceTransformer(MODEL_SIMILARITY_NAME)\n",
    "\n",
    "def return_similarity_sorted_array(right_answer, sentence_array, model=model_similarity):\n",
    "    \n",
    "    right_answer = right_answer.lower()\n",
    "    \n",
    "    embedding_right_answer = model.encode([right_answer], convert_to_tensor=True, device=device)\n",
    "    embedding_sentence_array = model.encode(sentence_array, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    # Using cosine scores to calculate\n",
    "    cosine_scores = util.pytorch_cos_sim(embedding_right_answer, embedding_sentence_array)\n",
    "    \n",
    "    sorted_indices = cosine_scores.argsort(descending=True)[0]\n",
    "    sorted_array = [sentence_array[i] for i in sorted_indices]\n",
    "    \n",
    "    return sorted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a9f59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# removing value with hash.\n",
    "# Because, from label-tagging before\n",
    "# Some data have a hash symbol, because\n",
    "# that data was part of a word fragment\n",
    "\n",
    "def remove_values_with_hash(arr):\n",
    "    return [item for item in arr if \"#\" not in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf6c62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve stopword from all language\n",
    "\n",
    "response = requests.get(URL_STOPWORD)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    stopword_data = response.json()\n",
    "else:\n",
    "    print(\"Failed to download JSON.\")\n",
    "\n",
    "stopword_data = set([item for sublist in list(stopword_data.values()) for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c043ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function just retrieve random word\n",
    "# of entire premise\n",
    "\n",
    "def select_random_word(text, answer, stopword_data=stopword_data):\n",
    "\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    # Filtering to remove stopword and punctuation\n",
    "    filtered_words = [word for word in words if word not in stopword_data and word not in string.punctuation]\n",
    "    \n",
    "    # If filtered words less than answer\n",
    "    # only take one word as random word\n",
    "    \n",
    "    if len(filtered_words) < len(answer.split()):\n",
    "        random_word = random.choice(filtered_words)\n",
    "    \n",
    "    # But, if filtered words NOT less than answer\n",
    "    # take a same length word as a random word\n",
    "    # with the same order as filtered words\n",
    "    \n",
    "    else:\n",
    "        start_index = random.randint(0, len(filtered_words) - len(answer.split()))\n",
    "        random_word_array = filtered_words[start_index : start_index + len(answer.split())]\n",
    "        random_word = ' '.join(random_word_array)\n",
    "    \n",
    "    return random_word.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d682a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for find the same order\n",
    "# of sequence, this function will used in\n",
    "# chunking domain, to classify whether an\n",
    "# answer is word or a sentence\n",
    "\n",
    "def find_order(list1, list2):\n",
    "    \n",
    "    matching_values = list()\n",
    "    i, j = 0, 0\n",
    "\n",
    "    while i < len(list1) and j < len(list2):\n",
    "        \n",
    "        if list1[i][0] == list2[j][0]:\n",
    "            matching_values.append(list2[j][1])\n",
    "            j += 1\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    if j == len(list2):\n",
    "        return \" \".join(matching_values)\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76d0afd1-e751-4cb8-ae22-1b7bb10a5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for grouping same tag-label \n",
    "# between answer and word (or sentence) in an entire premise\n",
    "\n",
    "def grouping_same_tag(tag_answers, tag_premises, same_tag_array, tag):\n",
    "    \n",
    "    # In NER, basicly you need to iterate\n",
    "    # to find a same tag-label\n",
    "    \n",
    "    if tag == \"ner\":\n",
    "        for tag_premise in tag_premises:\n",
    "\n",
    "            label_tag_premise = tag_premise[0]\n",
    "            word_premise = tag_premise[1]\n",
    "\n",
    "            for tag_answer in tag_answers:\n",
    "\n",
    "                label_tag_answer = tag_answer[0]\n",
    "\n",
    "                if label_tag_answer == label_tag_premise:\n",
    "                    same_tag_array.append(word_premise)\n",
    "                    \n",
    "    # In Chunking, slightly different\n",
    "    # you need to find subset for find the same order\n",
    "    # of sequence with find_order() function\n",
    "    \n",
    "    elif tag == \"chunking\":\n",
    "        \n",
    "        matching_words = find_order(tag_premises, tag_answers)\n",
    "        \n",
    "        # If there is a correct order of subset answer to premise, add it word\n",
    "        \n",
    "        if matching_words != None:\n",
    "            same_tag_array.append(matching_words)\n",
    "        \n",
    "        # If no matching words, use NER algorithm above\n",
    "        \n",
    "        else:\n",
    "            grouping_same_tag(tag_answers, tag_premises, same_tag_array, \"ner\")\n",
    "\n",
    "    # Still, filter value with hash\n",
    "    \n",
    "    return remove_values_with_hash(same_tag_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "598b3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# checking text if only\n",
    "# contain punctuation, no words at all \n",
    "\n",
    "def contains_only_punctuation(text):\n",
    "    return all(char in string.punctuation for char in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c20d3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# filter overlapping right answer and wrong answer\n",
    "# that provided in plausible answer\n",
    "\n",
    "def filtering_plausible_answer(answer, plausible_answer_array):\n",
    "    \n",
    "    if type(plausible_answer_array) == str: \n",
    "        plausible_answer_array = list(plausible_answer_array)\n",
    "    \n",
    "    answer = answer.lower()\n",
    "    \n",
    "    plausible_answer_array = [item.lower().strip() for item in plausible_answer_array]\n",
    "    plausible_answer_array = [string for string in plausible_answer_array if not contains_only_punctuation(string)]\n",
    "    plausible_answer_array = [remove_punctuation(text) for text in plausible_answer_array]\n",
    "    \n",
    "    final_plausible_answer_array = list()\n",
    "    answer_words = set(remove_punctuation(text) for text in answer.split())\n",
    "    \n",
    "    # For check overlapping answer, using set of word,\n",
    "    # and so, check for intersection\n",
    "    \n",
    "    for plausible_answer in plausible_answer_array:\n",
    "        plausible_answer_words = set(plausible_answer.split())\n",
    "        if not plausible_answer_words.intersection(answer_words):\n",
    "            final_plausible_answer_array.append(plausible_answer)\n",
    "    \n",
    "    return final_plausible_answer_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddb26145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# detecting number, date, time\n",
    "# to give plausible answer more\n",
    "# \"make sense\" answer\n",
    "\n",
    "def is_number(input_str):\n",
    "    pattern = r'^-?\\d+(\\.\\d+)?$'\n",
    "    return re.match(pattern, input_str) is not None\n",
    "\n",
    "def is_date(input_str):\n",
    "    pattern_1 = r'\\d{1,2} [A-Za-z]+(?: \\d{4})?'\n",
    "    pattern_2 = r'^\\d{4}-\\d{2}-\\d{2}$'\n",
    "    return (re.match(pattern_1, input_str) or re.match(pattern_2, input_str)) is not None\n",
    "\n",
    "def is_time(input_str):\n",
    "    pattern = r'^\\d{2}:\\d{2}:\\d{2}$'\n",
    "    return re.match(pattern, input_str) is not None\n",
    "\n",
    "def check_regex(right_answer, plausible_answer_array):\n",
    "    \n",
    "    if is_number(right_answer):\n",
    "        plausible_answer_array = [item for item in plausible_answer_array if is_number(item)]\n",
    "    \n",
    "    elif is_date(right_answer):\n",
    "        plausible_answer_array = [item for item in plausible_answer_array if is_date(item)]\n",
    "    \n",
    "    elif is_time(right_answer):\n",
    "        plausible_answer_array = [item for item in plausible_answer_array if is_time(item)]\n",
    "        \n",
    "    else:\n",
    "        plausible_answer_array = [item for item in plausible_answer_array if (not is_number(item) or \n",
    "                                                                              not is_date(item) or \n",
    "                                                                              not is_time(item)\n",
    "                                                                             )]\n",
    "    \n",
    "    return plausible_answer_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5369eced-0cee-4d0d-a436-89a97d2af242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# sorting similarity and\n",
    "# to give final wrong answer \n",
    "# and properties\n",
    "\n",
    "def sorting_similarity(data, right_answer, index, tag, plausible_answer_array, premise,\n",
    "                       NO_ANSWER_STATEMENT=NO_ANSWER_STATEMENT):\n",
    "\n",
    "    if tag == \"ner\": slice = 'same_ner_tag_answer'\n",
    "    elif tag == \"chunking\": slice = 'same_chunking_tag_answer'\n",
    "    else: slice = None\n",
    "\n",
    "    # Find all the sorted (by similarity) plausible wrong answer, \n",
    "    # and remove hask & punctuation only answer\n",
    "    \n",
    "    if slice != None:\n",
    "        wrong_answer_array = return_similarity_sorted_array(right_answer, data[slice][index])\n",
    "    \n",
    "    else:\n",
    "        wrong_answer_array = return_similarity_sorted_array(right_answer, plausible_answer_array)\n",
    "    \n",
    "    # Below, do the filtering to plausible answer\n",
    "    \n",
    "    plausible_answer_array = remove_values_with_hash(wrong_answer_array)\n",
    "    plausible_answer_array = filtering_plausible_answer(right_answer, plausible_answer_array)\n",
    "    plausible_answer_array = check_regex(right_answer, plausible_answer_array)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Only return the most similar to right_answer\n",
    "        wrong_answer = plausible_answer_array[0].strip()\n",
    "        \n",
    "        if tag == \"ner\": \n",
    "            properties = \"IDENTICAL NER labels were found, and the highest similarity score same NER array was selected\"\n",
    "        \n",
    "        elif tag == \"chunking\":\n",
    "            properties = \"IDENTICAL Chunking labels were found, and the highest similarity score from same Chunking array was selected\"\n",
    "        \n",
    "        else:\n",
    "            properties = \"NO CHUNKING labels were found, and the highest similarity score from plausible answer was selected\"\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        # Selecting wrong answer from random word in premise\n",
    "        wrong_answer = select_random_word(premise, right_answer)\n",
    "        \n",
    "        # If that random word is overlapping to right answer,\n",
    "        # iterate again until it is not overlap again\n",
    "        \n",
    "        while filtering_plausible_answer(right_answer, wrong_answer) == list():\n",
    "            wrong_answer = select_random_word(premise, right_answer)[0]\n",
    "            break\n",
    "        \n",
    "        # If it still detect overlapped right answer,\n",
    "        # just assign it with NO_ANSWER_STATEMENT.\n",
    "        # But, this condition is very-extraordinary\n",
    "        \n",
    "        else:\n",
    "            wrong_answer = NO_ANSWER_STATEMENT\n",
    "        \n",
    "        if tag == \"ner\": \n",
    "            properties = \"Detected (NER) wrong answer that is the SAME as the right answer, search random word from premise\"\n",
    "        \n",
    "        elif tag == \"chunking\":\n",
    "            properties = \"Detected (Chunking) wrong answer that is the SAME as the right answer, search random word from premise\"\n",
    "        \n",
    "        else:\n",
    "            properties = \"Detected (Random) wrong answer that is the SAME as the right answer, search random word from premise\"\n",
    "    \n",
    "    # Still need to check/assert the wrong answer\n",
    "    # and the plausible answer type\n",
    "    \n",
    "    assert isinstance(wrong_answer, str)\n",
    "    assert isinstance(plausible_answer_array, list)\n",
    "    \n",
    "    return wrong_answer, plausible_answer_array, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97759144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is the main idea to create wrong answer\n",
    "# Though, this function is helper function\n",
    "\n",
    "def create_wrong_answer(data, NO_ANSWER_STATEMENT=NO_ANSWER_STATEMENT):\n",
    "    \n",
    "    data['same_ner_tag_answer'] = \"\"\n",
    "    data['same_chunking_tag_answer'] = \"\"\n",
    "    data['wrong_answer'] = \"\"\n",
    "    data['no_answer'] = \"\"\n",
    "    data['plausible_answer_based_on_method'] = \"\"\n",
    "    data['properties'] = \"\"\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        \n",
    "        right_answer = data['answer'][i]\n",
    "        premise = data['premise'][i]\n",
    "\n",
    "        same_ner_tag_answer_array = list()\n",
    "        same_chunking_tag_answer_array = list()\n",
    "\n",
    "        ner_tag_answer = data['ner_tag_answer'][i]\n",
    "        ner_tag_premise = data['ner_tag_premise'][i]\n",
    "\n",
    "        chunking_tag_answer = data['chunking_tag_answer'][i]\n",
    "        chunking_tag_premise = data['chunking_tag_premise'][i]\n",
    "        \n",
    "        # If that row of data is unanswerable, do this, then continue\n",
    "        \n",
    "        if right_answer == \"\":\n",
    "            data['properties'][i] = \"Unanswerable question\"\n",
    "            data['wrong_answer'][i] = \"NULL\"\n",
    "            data['no_answer'][i] = \"NULL\"\n",
    "            data['plausible_answer_based_on_method'][i] = \"Unanswerable question\"\n",
    "            continue\n",
    "            \n",
    "        # Grouped with the same NER & Chunking group, between answer and word of premise\n",
    "        \n",
    "        data['same_ner_tag_answer'][i] = grouping_same_tag(ner_tag_answer,\n",
    "                                                           ner_tag_premise,\n",
    "                                                           same_ner_tag_answer_array, \"ner\")\n",
    "        \n",
    "        data['same_chunking_tag_answer'][i] = grouping_same_tag(chunking_tag_answer, \n",
    "                                                                chunking_tag_premise, \n",
    "                                                                same_chunking_tag_answer_array, \"chunking\")\n",
    "        \n",
    "        # Start to create wrong answer\n",
    "        plausible_answer_array = list()\n",
    "\n",
    "        # Perform NER classification\n",
    "        # If the NER of the right_answer can be detected, then calculate the distance using semantic \n",
    "        # similarity or word vectors between the right_answer and various possible wrong_answers with \n",
    "        # the same NER as the right_answer. Once done, proceed to the final wrong_answer.\n",
    "        \n",
    "        if data['same_ner_tag_answer'][i] != list():\n",
    "            wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                      i, \"ner\", plausible_answer_array, premise)\n",
    "            \n",
    "        # If the NER of the right_answer cannot be detected (NULL) or context/premise does not contain \n",
    "        # any of NER of right_answer, then the POS/Chunking of the right_answer will be identified.\n",
    "        \n",
    "        # Perform POS/Chunking classification\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # If the POS/Chunking of the right_answer can be detected, then calculate the distance \n",
    "            # using semantic similarity or word vectors between the right_answer and various possible \n",
    "            # wrong_answers with the same POS/Chunking as the right_answer. Once done, proceed to the \n",
    "            # final wrong_answer.\n",
    "            \n",
    "            if data['same_chunking_tag_answer'][i] != list():\n",
    "                wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                          i, \"chunking\", plausible_answer_array, premise)\n",
    "            \n",
    "            # If the POS/Chunking of the right_answer cannot be detected (NULL) or context/premise \n",
    "            # does not contain any of NER of right_answer, then the final wrong_answer will be chosen \n",
    "            # based on a plausible answer.\n",
    "            \n",
    "            else:\n",
    "                for chunking_tag in chunking_tag_premise:\n",
    "                    plausible_answer_array.append(chunking_tag[1])\n",
    "\n",
    "                wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                          i, \"none\", plausible_answer_array, premise)\n",
    "        data['properties'][i] = properties\n",
    "        data['wrong_answer'][i] = wrong_answer\n",
    "        data['no_answer'][i] = NO_ANSWER_STATEMENT\n",
    "        data['plausible_answer_based_on_method'][i] = plausible_answer_array\n",
    "            \n",
    "    return data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efed3dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7273aabe17984b288eb5af392f9fd789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1585dfd927404817ae4a081f1151dad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                                | 1/25 [00:01<00:45,  1.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236783fc2b6e43ce9a3ed875f604a7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a835a52332444c0a76efaae28dea34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d19e6c2749456a8bf75e75eb46b6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff5ce78681647a6b3fa1ca9e83b2389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f07ec332a94a6092ee352259cf9f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e1133d01fe4a2db4ff899e419afdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 5/25 [00:02<00:06,  3.21it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b2e5911835498ab3d558ca705b2998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56543d6ef944f88b8549f69c42d6794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baca1c595954ea79695a98df26cc5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6352b525ee51492e915687b28b8e0351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e299d8d903491bbd549bdd6cdef02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fa23e104a449478597c25d99f8b020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▉                                                         | 8/25 [00:02<00:03,  5.43it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a27a8e2e2a40ffbc988fa324af70d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c8436425724a11be9a8c3e601429bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa330e95663461bb319ec390a4d9928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e2bfa4e5e2403b990fed2cf1e1fae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f838b7568c1849899d96ccad79b81589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c010190b4b04bd8bd6abe3e15bfdad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 20/25 [00:02<00:00, 17.21it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87294c19e2994d218d64040632151a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d6225f71704ff4bb1db5552ded7224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7583db0a3b9a4aca860c4023069960c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f25843c0fba47a8a3aa4ed2c02ebc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b176c1a9aed441cd9b1d1de8c2a653d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf413534a8744d0be99887e054e8ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5808f1b685174e6195f1bdb51a51c6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aef23126ff045ddb86caac06ad1e8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00, 10.04it/s]\n",
      "  0%|                                                                                            | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84ebd0998d4dc08bc3bc6845f2db8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5769caf2b92a4fb9be6d19080bbc0c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f329ced80e648a0add3ccf12eaa4199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10f1a08887f4e7e9d206998a4e43c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▋                                                                             | 2/25 [00:00<00:01, 18.14it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1d2ba8423a452ab809a90d98854b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d600e8c616f46819611a6955928a967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db06d0c9325741a9858e1c495d4ac12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9670a4fcf73441f1a2618cd2bda22011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4d5a55698b4eaebfd26cfeccc2eda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4928adcd0de44b189f75fa00dee03176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 5/25 [00:00<00:00, 21.13it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d88ba9bdec40ad94cb4f51d40a66c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efec14fe3d764df6ae22f17d14f03c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008b8a77a9f64114b6a15d0b2270292b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7b353edab74853a9b7e3d5e7c4a14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb071b739d5641f4bc92824b8bf7504e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4151230de24141a4e638f3dc977577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 10/25 [00:00<00:00, 28.82it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953bf735eb6640b4ba61b7fb8595acfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660b8d9668e74db2809c0892bca0ba2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90660deb4308423f8d4a5bdaa3ee787f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3228a75c75f0493c86801ef3a3192228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d26a603867e422da1d147b38e21404f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deac1b21509a438da0f2c6fda4f09200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████▍                          | 17/25 [00:00<00:00, 38.25it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152af38f916f49c49b546f01895a1645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5aa1e2fd1f42d3ac76462348b5bb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ee706bfbc94d25b997ebb9b211f9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d0191ec9974a14a43e3abec2c86741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b730119dd08464fab18be0477eb131c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e69e4effea4770ab182c7bcee13db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████████████████████▎      | 23/25 [00:00<00:00, 40.39it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8ffdd4e1e84c8aaa7acb81c56e6971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373815782e064622900371cb0a57474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 36.09it/s]\n",
      "  0%|                                                                                            | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791475efe37d4da2baaa48b7cb972795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a9c597bf07492eaa7af2f6409bf319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc4f74a485f4660b0cae45222a255f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d860e8c30b47b9a6c4a874b7d9482e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████                                                                          | 3/25 [00:00<00:00, 28.73it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f97c004512547ae959ea678f2fd2913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5c2ee54758498d8a3eb127f20ef70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bfbdbf424b414bbd2155c45d41b642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6c72f1c57d498d99a0c71442656c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d6614232984975ae1011e47d22dd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527d1375a5764dedafc95c557fb18fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 10/25 [00:00<00:00, 32.77it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8a5d900afc402589261b9e78b0e696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f3cd29501e4fe6b0366e16c593f9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c75ad163024566845455c92c461751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada91898dcba4dd592fe0c39940e2545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c10dd73f9db498ab29dcf9f341fa456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80b6ea9cc91452c8ad566651f67e2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████▍                                    | 14/25 [00:00<00:00, 30.60it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68c22a79d87451bbe4f8679ecf8f101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365c5f466ea642d69e433ed544b730f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb38b94351842169705db36d2ac5bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae59f2447c44f91a9428e3a7a399c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bbe845a79d423e85be9ce7fb92097b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a08369b82904174b07fdb48684540f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 42.07it/s]\n"
     ]
    }
   ],
   "source": [
    "data_nli_train_df = create_wrong_answer(data_nli_train_df)\n",
    "data_nli_val_df = create_wrong_answer(data_nli_val_df)\n",
    "data_nli_test_df = create_wrong_answer(data_nli_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe61c82",
   "metadata": {},
   "source": [
    "# Split to two dataset: right dataset & wrong dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9d6c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is just only\n",
    "# for aesthetics of column number\n",
    "\n",
    "def move_to_column_number(data, column_name=\"hypothesis\", column_num=3):\n",
    "\n",
    "    cols = list(data.columns)\n",
    "    cols.remove(column_name)\n",
    "    cols.insert(column_num, column_name)\n",
    "\n",
    "    data = data[cols]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56880d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating answerable right (entailment label) dataset\n",
    "\n",
    "columns_to_exclude = ['wrong_answer', 'no_answer']\n",
    "\n",
    "data_nli_answerable_right_train_df = data_nli_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_answerable_right_val_df = data_nli_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_answerable_right_test_df = data_nli_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_answerable_right_train_df = data_nli_answerable_right_train_df[data_nli_answerable_right_train_df['answer'] != '']\n",
    "data_nli_answerable_right_val_df = data_nli_answerable_right_val_df[data_nli_answerable_right_val_df['answer'] != '']\n",
    "data_nli_answerable_right_test_df = data_nli_answerable_right_test_df[data_nli_answerable_right_test_df['answer'] != '']\n",
    "\n",
    "data_nli_answerable_right_train_df = data_nli_answerable_right_train_df.reset_index(drop=True)\n",
    "data_nli_answerable_right_val_df = data_nli_answerable_right_val_df.reset_index(drop=True)\n",
    "data_nli_answerable_right_test_df = data_nli_answerable_right_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "232c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating answerable wrong (contradiction label) dataset\n",
    "\n",
    "columns_to_exclude = ['answer', 'no_answer']\n",
    "\n",
    "data_nli_answerable_wrong_train_df = data_nli_train_df[data_nli_train_df['answer'] != '']\n",
    "data_nli_answerable_wrong_val_df = data_nli_val_df[data_nli_val_df['answer'] != '']\n",
    "data_nli_answerable_wrong_test_df = data_nli_test_df[data_nli_test_df['answer'] != '']\n",
    "\n",
    "data_nli_answerable_wrong_train_df = data_nli_answerable_wrong_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_answerable_wrong_val_df = data_nli_answerable_wrong_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_answerable_wrong_test_df = data_nli_answerable_wrong_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_answerable_wrong_train_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "data_nli_answerable_wrong_val_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "data_nli_answerable_wrong_test_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "\n",
    "data_nli_answerable_wrong_train_df = data_nli_answerable_wrong_train_df.reset_index(drop=True)\n",
    "data_nli_answerable_wrong_val_df = data_nli_answerable_wrong_val_df.reset_index(drop=True)\n",
    "data_nli_answerable_wrong_test_df = data_nli_answerable_wrong_test_df.reset_index(drop=True)\n",
    "\n",
    "data_nli_answerable_wrong_train_df = move_to_column_number(data_nli_answerable_wrong_train_df, \"answer\", 2)\n",
    "data_nli_answerable_wrong_val_df = move_to_column_number(data_nli_answerable_wrong_val_df, \"answer\", 2)\n",
    "data_nli_answerable_wrong_test_df = move_to_column_number(data_nli_answerable_wrong_test_df, \"answer\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "297a7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unanswerable right (entailment label) and no-answer dataset\n",
    "\n",
    "columns_to_exclude = ['wrong_answer', 'no_answer']\n",
    "\n",
    "data_nli_unanswerable_right_train_df = data_nli_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_unanswerable_right_val_df = data_nli_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_unanswerable_right_test_df = data_nli_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_unanswerable_right_train_df = data_nli_unanswerable_right_train_df[data_nli_unanswerable_right_train_df['answer'] == '']\n",
    "data_nli_unanswerable_right_val_df = data_nli_unanswerable_right_val_df[data_nli_unanswerable_right_val_df['answer'] == '']\n",
    "data_nli_unanswerable_right_test_df = data_nli_unanswerable_right_test_df[data_nli_unanswerable_right_test_df['answer'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef5df327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unanswerable wrong (contradiction label) and no-answer dataset\n",
    "\n",
    "columns_to_exclude = ['answer', 'wrong_answer']\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = data_nli_train_df[data_nli_train_df['answer'] != '']\n",
    "data_nli_unanswerable_wrong_val_df = data_nli_val_df[data_nli_val_df['answer'] != '']\n",
    "data_nli_unanswerable_wrong_test_df = data_nli_test_df[data_nli_test_df['answer'] != '']\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = data_nli_unanswerable_wrong_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_unanswerable_wrong_val_df = data_nli_unanswerable_wrong_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_unanswerable_wrong_test_df = data_nli_unanswerable_wrong_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df.rename(columns={'no_answer': 'answer'}, inplace=True)\n",
    "data_nli_unanswerable_wrong_val_df.rename(columns={'no_answer': 'answer'}, inplace=True)\n",
    "data_nli_unanswerable_wrong_test_df.rename(columns={'no_answer': 'answer'}, inplace=True)\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = move_to_column_number(data_nli_unanswerable_wrong_train_df, \"answer\", 2)\n",
    "data_nli_unanswerable_wrong_val_df = move_to_column_number(data_nli_unanswerable_wrong_val_df, \"answer\", 2)\n",
    "data_nli_unanswerable_wrong_test_df = move_to_column_number(data_nli_unanswerable_wrong_test_df, \"answer\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3b3c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rather than duplicating the no-answer statement, \n",
    "# it's better to remove the excessing row ones.\n",
    "\n",
    "def balancing_data(data1, data2):\n",
    "    \n",
    "    if len(data1) > len(data2):\n",
    "        data1 = data1.sample(n=len(data2))\n",
    "    \n",
    "    elif len(data1) < len(data2):\n",
    "        data2 = data2.sample(n=len(data1))\n",
    "        \n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8210383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_right_train_df, data_nli_unanswerable_wrong_train_df = balancing_data(data_nli_unanswerable_right_train_df,\n",
    "                                                                                            data_nli_unanswerable_wrong_train_df)\n",
    "\n",
    "data_nli_unanswerable_right_val_df, data_nli_unanswerable_wrong_val_df = balancing_data(data_nli_unanswerable_right_val_df,\n",
    "                                                                                        data_nli_unanswerable_wrong_val_df)\n",
    "\n",
    "data_nli_unanswerable_right_test_df, data_nli_unanswerable_wrong_test_df = balancing_data(data_nli_unanswerable_right_test_df,\n",
    "                                                                                        data_nli_unanswerable_wrong_test_df)\n",
    "\n",
    "# Still need to reset index of DataFrame\n",
    "\n",
    "data_nli_unanswerable_right_train_df = data_nli_unanswerable_right_train_df.reset_index(drop=True)\n",
    "data_nli_unanswerable_right_val_df = data_nli_unanswerable_right_val_df.reset_index(drop=True)\n",
    "data_nli_unanswerable_right_test_df = data_nli_unanswerable_right_test_df.reset_index(drop=True)\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = data_nli_unanswerable_wrong_train_df.reset_index(drop=True)\n",
    "data_nli_unanswerable_wrong_val_df = data_nli_unanswerable_wrong_val_df.reset_index(drop=True)\n",
    "data_nli_unanswerable_wrong_test_df = data_nli_unanswerable_wrong_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d4f9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTAILMENT ANSWERABLE\n",
      "TRAIN: 14\n",
      "VAL: 15\n",
      "TEST: 11\n",
      "\n",
      "CONTRADICTION ANSWERABLE\n",
      "TRAIN: 14\n",
      "VAL: 15\n",
      "TEST: 11\n",
      "\n",
      "ENTAILMENT UN-ANSWERABLE\n",
      "TRAIN: 11\n",
      "VAL: 10\n",
      "TEST: 11\n",
      "\n",
      "CONTRADICTION UN-ANSWERABLE\n",
      "TRAIN: 11\n",
      "VAL: 10\n",
      "TEST: 11\n"
     ]
    }
   ],
   "source": [
    "# For debug purpose\n",
    "\n",
    "print(\"ENTAILMENT ANSWERABLE\")\n",
    "print(\"TRAIN:\", len(data_nli_answerable_right_train_df))\n",
    "print(\"VAL:\", len(data_nli_answerable_right_val_df))\n",
    "print(\"TEST:\", len(data_nli_answerable_right_test_df))\n",
    "print()\n",
    "\n",
    "print(\"CONTRADICTION ANSWERABLE\")\n",
    "print(\"TRAIN:\", len(data_nli_answerable_wrong_train_df))\n",
    "print(\"VAL:\", len(data_nli_answerable_wrong_val_df))\n",
    "print(\"TEST:\", len(data_nli_answerable_wrong_test_df))\n",
    "print()\n",
    "\n",
    "print(\"ENTAILMENT UN-ANSWERABLE\")\n",
    "print(\"TRAIN:\", len(data_nli_unanswerable_right_train_df))\n",
    "print(\"VAL:\", len(data_nli_unanswerable_right_val_df))\n",
    "print(\"TEST:\", len(data_nli_unanswerable_right_test_df))\n",
    "print()\n",
    "\n",
    "print(\"CONTRADICTION UN-ANSWERABLE\")\n",
    "print(\"TRAIN:\", len(data_nli_unanswerable_wrong_train_df))\n",
    "print(\"VAL:\", len(data_nli_unanswerable_wrong_val_df))\n",
    "print(\"TEST:\", len(data_nli_unanswerable_wrong_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e6f08",
   "metadata": {},
   "source": [
    "# Convert question-answer pair to hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9cb9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe we can try this approach\n",
    "\n",
    "#nlp_tools_paraphraser = pipeline(task = TASK_PARAPHRASER_NAME, \n",
    "#                     model = MODEL_PARAPHRASER_NAME, \n",
    "#                     tokenizer = AutoTokenizer.from_pretrained(MODEL_PARAPHRASER_NAME, \n",
    "#                                                               model_max_length=512, \n",
    "#                                                               truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01d0b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# retrieve hypothesis from\n",
    "# question and answer\n",
    "\n",
    "def convert_question_and_answer_to_hypothesis(data, NO_ANSWER_STATEMENT=NO_ANSWER_STATEMENT):\n",
    "    \n",
    "    data['hypothesis'] = \"\"\n",
    "    hypothesis_array = list()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if data['answer'][i] != \"\" and type(data['answer'][i]) == str:\n",
    "            hypothesis_array.append(data['question'][i] + ' ' + data['answer'][i])\n",
    "        else:\n",
    "            hypothesis_array.append(data['question'][i] + ' ' + NO_ANSWER_STATEMENT)\n",
    "        \n",
    "        # Use this to decline no-answer-warning properties\n",
    "        #hypothesis_array.append(data['question'][i] + ' ' + data['answer'][i])\n",
    "        \n",
    "        # Use this to use paraphraser\n",
    "        #hypothesis_array.append(str(nlp_tools_paraphraser(data['question'][i] + ' ' + data['answer'][i])[0]['generated_text']))\n",
    "    \n",
    "    data['hypothesis'] = hypothesis_array\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acf340e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_answerable_right_train_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_right_train_df)\n",
    "data_nli_answerable_right_val_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_right_val_df)\n",
    "data_nli_answerable_right_test_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_right_test_df)\n",
    "\n",
    "data_nli_answerable_right_train_df = move_to_column_number(data_nli_answerable_right_train_df, \"hypothesis\", 3)\n",
    "data_nli_answerable_right_val_df = move_to_column_number(data_nli_answerable_right_val_df, \"hypothesis\", 3)\n",
    "data_nli_answerable_right_test_df = move_to_column_number(data_nli_answerable_right_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20241aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_answerable_wrong_train_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_wrong_train_df)\n",
    "data_nli_answerable_wrong_val_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_wrong_val_df)\n",
    "data_nli_answerable_wrong_test_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_wrong_test_df)\n",
    "\n",
    "data_nli_answerable_wrong_train_df = move_to_column_number(data_nli_answerable_wrong_train_df, \"hypothesis\", 3)\n",
    "data_nli_answerable_wrong_val_df = move_to_column_number(data_nli_answerable_wrong_val_df, \"hypothesis\", 3)\n",
    "data_nli_answerable_wrong_test_df = move_to_column_number(data_nli_answerable_wrong_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8706a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_right_train_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_right_train_df)\n",
    "data_nli_unanswerable_right_val_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_right_val_df)\n",
    "data_nli_unanswerable_right_test_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_right_test_df)\n",
    "\n",
    "data_nli_unanswerable_right_train_df = move_to_column_number(data_nli_unanswerable_right_train_df, \"hypothesis\", 3)\n",
    "data_nli_unanswerable_right_val_df = move_to_column_number(data_nli_unanswerable_right_val_df, \"hypothesis\", 3)\n",
    "data_nli_unanswerable_right_test_df = move_to_column_number(data_nli_unanswerable_right_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5d28ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_wrong_train_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_wrong_train_df)\n",
    "data_nli_unanswerable_wrong_val_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_wrong_val_df)\n",
    "data_nli_unanswerable_wrong_test_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_wrong_test_df)\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = move_to_column_number(data_nli_unanswerable_wrong_train_df, \"hypothesis\", 3)\n",
    "data_nli_unanswerable_wrong_val_df = move_to_column_number(data_nli_unanswerable_wrong_val_df, \"hypothesis\", 3)\n",
    "data_nli_unanswerable_wrong_test_df = move_to_column_number(data_nli_unanswerable_wrong_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c4ace",
   "metadata": {},
   "source": [
    "# Assign the label: entailment & contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45df14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_answerable_right_train_df['label'] = 'entailment'\n",
    "data_nli_answerable_right_val_df['label'] = 'entailment'\n",
    "data_nli_answerable_right_test_df['label'] = 'entailment'\n",
    "\n",
    "data_nli_answerable_right_train_df = move_to_column_number(data_nli_answerable_right_train_df, \"label\", 4)\n",
    "data_nli_answerable_right_val_df = move_to_column_number(data_nli_answerable_right_val_df, \"label\", 4)\n",
    "data_nli_answerable_right_test_df = move_to_column_number(data_nli_answerable_right_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02098578",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_answerable_wrong_train_df['label'] = 'contradiction'\n",
    "data_nli_answerable_wrong_val_df['label'] = 'contradiction'\n",
    "data_nli_answerable_wrong_test_df['label'] = 'contradiction'\n",
    "\n",
    "data_nli_answerable_wrong_train_df = move_to_column_number(data_nli_answerable_wrong_train_df, \"label\", 4)\n",
    "data_nli_answerable_wrong_val_df = move_to_column_number(data_nli_answerable_wrong_val_df, \"label\", 4)\n",
    "data_nli_answerable_wrong_test_df = move_to_column_number(data_nli_answerable_wrong_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "198ebcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_right_train_df['label'] = 'entailment'\n",
    "data_nli_unanswerable_right_val_df['label'] = 'entailment'\n",
    "data_nli_unanswerable_right_test_df['label'] = 'entailment'\n",
    "\n",
    "data_nli_unanswerable_right_train_df = move_to_column_number(data_nli_unanswerable_right_train_df, \"label\", 4)\n",
    "data_nli_unanswerable_right_val_df = move_to_column_number(data_nli_unanswerable_right_val_df, \"label\", 4)\n",
    "data_nli_unanswerable_right_test_df = move_to_column_number(data_nli_unanswerable_right_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d219b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_wrong_train_df['label'] = 'contradiction'\n",
    "data_nli_unanswerable_wrong_val_df['label'] = 'contradiction'\n",
    "data_nli_unanswerable_wrong_test_df['label'] = 'contradiction'\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = move_to_column_number(data_nli_unanswerable_wrong_train_df, \"label\", 4)\n",
    "data_nli_unanswerable_wrong_val_df = move_to_column_number(data_nli_unanswerable_wrong_val_df, \"label\", 4)\n",
    "data_nli_unanswerable_wrong_test_df = move_to_column_number(data_nli_unanswerable_wrong_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f431144",
   "metadata": {},
   "source": [
    "# Concat the right and wrong NLI to one NLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "165d72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df_final = pd.concat([data_nli_answerable_right_train_df, \n",
    "                                     data_nli_answerable_wrong_train_df,\n",
    "                                     data_nli_unanswerable_right_train_df,\n",
    "                                     data_nli_unanswerable_wrong_train_df], axis=0, ignore_index=True)\n",
    "\n",
    "data_nli_val_df_final = pd.concat([data_nli_answerable_right_val_df, \n",
    "                                   data_nli_answerable_wrong_val_df,\n",
    "                                   data_nli_unanswerable_right_val_df,\n",
    "                                   data_nli_unanswerable_wrong_val_df], axis=0, ignore_index=True)\n",
    "\n",
    "data_nli_test_df_final = pd.concat([data_nli_answerable_right_test_df, \n",
    "                                    data_nli_answerable_wrong_test_df,\n",
    "                                    data_nli_unanswerable_right_test_df,\n",
    "                                    data_nli_unanswerable_wrong_test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a980a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug purpose,\n",
    "# you can modify it too\n",
    "\n",
    "def debug_data(data):\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        print(f\"Iteration: {i}\")\n",
    "        print(f\"Answer: {data['answer'][i]}\")\n",
    "        \n",
    "        print(\"NER\")\n",
    "        print(data['ner_tag_answer'][i])\n",
    "        print(data['ner_tag_premise'][i])\n",
    "        \n",
    "        print(\"Chunking\")\n",
    "        print(data['chunking_tag_answer'][i])\n",
    "        print(data['chunking_tag_premise'][i])\n",
    "        print()\n",
    "\n",
    "# debug_data(data_nli_train_df_final)\n",
    "# debug_data(data_nli_val_df_final)\n",
    "# debug_data(data_nli_test_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2e0b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN FINAL\n",
      "50\n",
      "\n",
      "VAL FINAL\n",
      "50\n",
      "\n",
      "TEST FINAL\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "# For debug purpose\n",
    "\n",
    "print(\"TRAIN FINAL\")\n",
    "print(len(data_nli_train_df_final))\n",
    "print()\n",
    "\n",
    "print(\"VAL FINAL\")\n",
    "print(len(data_nli_val_df_final))\n",
    "print()\n",
    "\n",
    "print(\"TEST FINAL\")\n",
    "print(len(data_nli_test_df_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea9097",
   "metadata": {},
   "source": [
    "# Convert to DataFrame format to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "794bedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df_final.to_csv(f\"{NAME}_nli_train_df.csv\", index=False)\n",
    "data_nli_val_df_final.to_csv(f\"{NAME}_nli_val_df.csv\", index=False)\n",
    "data_nli_test_df_final.to_csv(f\"{NAME}_nli_test_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85460bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM FINISHED\n"
     ]
    }
   ],
   "source": [
    "print(\"PROGRAM FINISHED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
