{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9171daa4",
   "metadata": {},
   "source": [
    "# Define tool and model of the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a063a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 29 14:05:40 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   46C    P0   150W / 300W |  31842MiB / 32510MiB |     87%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   52C    P0   109W / 300W |  30372MiB / 32510MiB |     27%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   49C    P0   116W / 300W |  29084MiB / 32510MiB |     84%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    61W / 300W |    962MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    60W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    41W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f10e14",
   "metadata": {},
   "source": [
    "Below, it is some settings to run in my local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59d867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e983e0",
   "metadata": {},
   "source": [
    "You can tweak your settings too in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41849a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "NAME = \"idk-mrc\"\n",
    "NO_ANSWER_STATEMENT = \"Tidak ada jawaban\"\n",
    "\n",
    "TASK_NER_NAME = \"ner\"\n",
    "MODEL_NER_NAME = \"ageng-anugrah/indobert-large-p2-finetuned-ner\"\n",
    "\n",
    "TASK_CHUNKING_NAME = \"token-classification\"\n",
    "MODEL_CHUNKING_NAME = \"ageng-anugrah/indobert-large-p2-finetuned-chunking\"\n",
    "\n",
    "MODEL_SIMILARITY_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\n",
    "URL_STOPWORD = \"https://raw.githubusercontent.com/6/stopwords-json/master/stopwords-all.json\"\n",
    "\n",
    "TASK_PARAPHRASER_NAME = \"text2text-generation\"\n",
    "MODEL_PARAPHRASER_NAME = \"\"\n",
    "\n",
    "# Uncomment sys.maxsize to create all of the data, \n",
    "# else if you want to debugging\n",
    "\n",
    "# SAMPLE = sys.maxsize\n",
    "SAMPLE = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9fdd4",
   "metadata": {},
   "source": [
    "# Import anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26d313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "import string\n",
    "import contextlib\n",
    "import gc\n",
    "import random\n",
    "import string\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback, \n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079cd27c",
   "metadata": {},
   "source": [
    "# Retrieve QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f426d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM STARTED\n"
     ]
    }
   ],
   "source": [
    "print(\"PROGRAM STARTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dafbf0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset idk_mrc (/root/.cache/huggingface/datasets/idk_mrc/idk_mrc_source/1.0.0/cf468d86fa7341e69998db1449851672ebfb4fa46036929d66b9de15c421334f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ff3d17908f486b963c383f12c2b2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3659/3659 [00:16<00:00, 227.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 358/358 [00:01<00:00, 267.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 378/378 [00:01<00:00, 256.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 9332\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 764\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas = conhelps.filtered(lambda x: 'idk_mrc' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas['train'])\n",
    "df_validation = pd.DataFrame(data_qas['validation'])\n",
    "df_test = pd.DataFrame(data_qas['test'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_train['context']))):\n",
    "    for j in df_train[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                           \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                           \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                           ignore_index=True)\n",
    "        else:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": str(), \n",
    "                                                           \"answer_start\": 0, \n",
    "                                                           \"answer_end\": 0}}, \n",
    "                                                           ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_validation['context']))):\n",
    "    for j in df_validation[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)        \n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_test = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_test['context']))):\n",
    "    for j in df_test[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "test_dataset = Dataset.from_dict(new_df_test)\n",
    "\n",
    "data_qas = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})\n",
    "data_qas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae908a",
   "metadata": {},
   "source": [
    "# Convert to NLI, with hypothesis being just do concat question & answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c79ac",
   "metadata": {},
   "source": [
    "## Convert Dataset to DataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b342b8ce-41f9-4714-84a5-1697cfee1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42, the answer to life the universe and everything\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275dc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to training all of the data (prod),\n",
    "# this code will convert to DataFrame.\n",
    "# However, if you want to debug (not-prod),\n",
    "# this code will convert SAMPLE of your DataFrame\n",
    "\n",
    "if SAMPLE == sys.maxsize:\n",
    "    data_qas_train_df = pd.DataFrame(data_qas[\"train\"][:SAMPLE])\n",
    "    data_qas_val_df = pd.DataFrame(data_qas[\"validation\"][:SAMPLE])\n",
    "    data_qas_test_df = pd.DataFrame(data_qas[\"test\"][:SAMPLE])\n",
    "\n",
    "else:\n",
    "    data_qas_train_df = (pd.DataFrame(data_qas[\"train\"])).sample(n=SAMPLE, random_state=seed_value)\n",
    "    data_qas_val_df = (pd.DataFrame(data_qas[\"validation\"])).sample(n=SAMPLE, random_state=seed_value)\n",
    "    data_qas_test_df = (pd.DataFrame(data_qas[\"test\"])).sample(n=SAMPLE, random_state=seed_value)\n",
    "\n",
    "    data_qas_train_df = data_qas_train_df.reset_index(drop=True)\n",
    "    data_qas_val_df = data_qas_val_df.reset_index(drop=True)\n",
    "    data_qas_test_df = data_qas_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bbf0b",
   "metadata": {},
   "source": [
    "## Retrieve answer text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0424485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retrieve answer text\n",
    "# Because, we do not use answer_start\n",
    "# and answer_end\n",
    "\n",
    "def retrieve_answer_text(data):\n",
    "    for i in range(len(data)):\n",
    "        data['answer'][i] = data['answer'][i]['text']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b1a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qas_train_df = retrieve_answer_text(data_qas_train_df)\n",
    "data_qas_val_df = retrieve_answer_text(data_qas_val_df)\n",
    "data_qas_test_df = retrieve_answer_text(data_qas_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881d292",
   "metadata": {},
   "source": [
    "## Create NLI dataset from copy of QA dataset above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8808af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df = data_qas_train_df.copy()\n",
    "data_nli_val_df = data_qas_val_df.copy()\n",
    "data_nli_test_df = data_qas_test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83072dc4",
   "metadata": {},
   "source": [
    "## Convert context pair to premise (only renaming column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1562622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming it, just for consistency\n",
    "\n",
    "data_nli_train_df = data_nli_train_df.rename(columns={\"context\": \"premise\"})\n",
    "data_nli_val_df = data_nli_val_df.rename(columns={\"context\": \"premise\"})\n",
    "data_nli_test_df = data_nli_test_df.rename(columns={\"context\": \"premise\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33986ce",
   "metadata": {},
   "source": [
    "# Add contradiction label cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095664cc",
   "metadata": {},
   "source": [
    "## Import pipeline to create contradiction cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8850d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tools_ner = pipeline(task = TASK_NER_NAME, \n",
    "                     model = MODEL_NER_NAME, \n",
    "                     tokenizer = AutoTokenizer.from_pretrained(MODEL_NER_NAME, \n",
    "                                                               model_max_length=512, \n",
    "                                                               truncation=True),\n",
    "                     aggregation_strategy = 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e84dda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tools_chunking = pipeline(task = TASK_CHUNKING_NAME, \n",
    "                     model = MODEL_CHUNKING_NAME, \n",
    "                     tokenizer = AutoTokenizer.from_pretrained(MODEL_CHUNKING_NAME, \n",
    "                                                               model_max_length=512, \n",
    "                                                               truncation=True),\n",
    "                     aggregation_strategy = 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae4a76",
   "metadata": {},
   "source": [
    "## Add NER and chunking tag column in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a68f3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code useful for cleaning the data (text)\n",
    "\n",
    "def remove_space_after_number_and_punctuation(text):\n",
    "    pattern = r'(\\d+)\\s*([.,])\\s*(?=\\S|$)'\n",
    "    cleaned_text = re.sub(pattern, r'\\1\\2', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee495118-61e1-4603-9194-690c0ae737c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code useful for tagging the entire premise\n",
    "# with NER and chunking tools\n",
    "\n",
    "def add_premise_tag(data, tag, index, premise_array, ner=nlp_tools_ner, chunking=nlp_tools_chunking):\n",
    "\n",
    "    if tag == \"ner\": tools=ner\n",
    "    else: tools=chunking\n",
    "    \n",
    "    # If the tools detected nothing, retrieve NO TOKEN DETECTED\n",
    "    if len(tools(data['premise'][index])) == 0:\n",
    "        premise_array.append(\"NO TOKEN DETECTED\")\n",
    "    \n",
    "    # Else if, the tools detected something, retrieve all of the entity and the word associated\n",
    "    else:\n",
    "        for j in tools(data['premise'][index]):\n",
    "            tag_premise = (j['entity_group'], remove_space_after_number_and_punctuation(j['word']))\n",
    "            premise_array.append(tag_premise)\n",
    "\n",
    "    return premise_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8de4a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for clean the text off punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c888804-88ec-4858-ba18-131545ee6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code useful for tagging the entire answer\n",
    "# with NER and chunking tools\n",
    "\n",
    "def add_answer_tag(answer, tag, premise_array, ner=nlp_tools_ner, chunking=nlp_tools_chunking):\n",
    "\n",
    "    if tag == \"ner\": tools=ner\n",
    "    else: tools=chunking\n",
    "\n",
    "    tag_answer_list = list()\n",
    "    \n",
    "    if tag == \"ner\":\n",
    "        \n",
    "        # If tools in premise detecting some token\n",
    "        \n",
    "        if len(premise_array) != 0:\n",
    "            \n",
    "            for i in premise_array:\n",
    "                \n",
    "                # Extract the label and the word of premise\n",
    "                \n",
    "                label_from_premise_tag = i[0]\n",
    "                word_from_premise_tag = remove_space_after_number_and_punctuation(i[1])\n",
    "\n",
    "                # With assumption, that I do not dividing label when\n",
    "                # there is more than one label in one word answer.\n",
    "                # Instead, I give a NULL.\n",
    "\n",
    "                if word_from_premise_tag.lower() == answer.lower():\n",
    "                    tag_answer = (label_from_premise_tag, word_from_premise_tag)\n",
    "                    break\n",
    "\n",
    "                # Or, I could do this: to reducing NULL label \n",
    "                # with subset of string not really with the entire string.\n",
    "                \n",
    "                elif answer.lower() in word_from_premise_tag:\n",
    "                    tag_answer = (label_from_premise_tag, answer.lower())\n",
    "                    break\n",
    "                \n",
    "                # Then, if you still do not find the word, NULL given\n",
    "                \n",
    "                else:\n",
    "                    tag_answer = (\"NULL\", answer)\n",
    "            \n",
    "            tag_answer_list.append(tag_answer)\n",
    "\n",
    "        # If tools in premise NOT detecting some token, NULL given\n",
    "        \n",
    "        else:\n",
    "            tag_answer = (\"NULL\", answer)\n",
    "            tag_answer_list.append(tag_answer)\n",
    "    \n",
    "    elif tag == \"chunking\":\n",
    "        \n",
    "        # In chunking, it's slightly different because \n",
    "        # the basic assumption is that there are no NULL chunks,\n",
    "        # so it will capture all the chunk labels.\n",
    "        \n",
    "        retrieved_from_tools = tools(answer)\n",
    "\n",
    "        if len(retrieved_from_tools) != 0:\n",
    "            \n",
    "            for i in retrieved_from_tools:\n",
    "                tag_answer = (i['entity_group'], i['word'])\n",
    "                tag_answer_list.append(tag_answer)\n",
    "        \n",
    "        # So, it rarely going down there\n",
    "        # But, if really going down there\n",
    "        # from basic assumption, there is no NULL in chunking\n",
    "        # We can check subset of the sentence from premise,\n",
    "        # if found, we can take that particular label\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in premise_array:\n",
    "                \n",
    "                # Extract the label and the word of premise\n",
    "                \n",
    "                label_from_premise_tag = i[0]\n",
    "                word_from_premise_tag = remove_space_after_number_and_punctuation(i[1])\n",
    "                \n",
    "                # Take label from subset of sentence from premise\n",
    "                \n",
    "                if answer.lower() in word_from_premise_tag:\n",
    "                    tag_answer = (label_from_premise_tag, answer.lower())\n",
    "                    tag_answer_list.append(tag_answer)\n",
    "                    break\n",
    "            \n",
    "            # Use for and then direct else (for-else),\n",
    "            # if for-loop above not getting the break statement\n",
    "            \n",
    "            else:\n",
    "                tag_answer = (\"NULL\", answer)\n",
    "                tag_answer_list.append(tag_answer)\n",
    "        \n",
    "    return tag_answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4967bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper code to run\n",
    "# process for add tag to entire premise\n",
    "# and answer\n",
    "\n",
    "def add_ner_and_chunking_all_tag(data):\n",
    "    \n",
    "    data['ner_tag_answer'] = \"\"\n",
    "    data['chunking_tag_answer'] = \"\"\n",
    "    \n",
    "    data['ner_tag_premise'] = \"\"\n",
    "    data['chunking_tag_premise'] = \"\"\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        \n",
    "        answer = data['answer'][i]\n",
    "        premise = data['premise'][i]\n",
    "        \n",
    "        ner_premise_array = list()\n",
    "        chunking_premise_array = list()\n",
    "                                                \n",
    "        data['ner_tag_premise'][i] = add_premise_tag(data, \"ner\", i, ner_premise_array)\n",
    "        data['chunking_tag_premise'][i] = add_premise_tag(data, \"chunking\", i, chunking_premise_array)\n",
    "        \n",
    "        data['ner_tag_answer'][i] = add_answer_tag(answer, \"ner\", data['ner_tag_premise'][i])\n",
    "        data['chunking_tag_answer'][i] = add_answer_tag(answer, \"chunking\", data['chunking_tag_premise'][i])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25cad8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [01:37<00:00,  3.88s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [01:19<00:00,  3.19s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [01:06<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "data_nli_train_df = add_ner_and_chunking_all_tag(data_nli_train_df)\n",
    "data_nli_val_df = add_ner_and_chunking_all_tag(data_nli_val_df)\n",
    "data_nli_test_df = add_ner_and_chunking_all_tag(data_nli_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49a3e4",
   "metadata": {},
   "source": [
    "# Create wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0d00a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# This function useful for sorting the closest distance\n",
    "# by using embedding\n",
    "\n",
    "model_similarity = SentenceTransformer(MODEL_SIMILARITY_NAME)\n",
    "\n",
    "def return_similarity_sorted_array(right_answer, sentence_array, model=model_similarity):\n",
    "    \n",
    "    right_answer = right_answer.lower()\n",
    "    \n",
    "    embedding_right_answer = model.encode([right_answer], convert_to_tensor=True, device=device)\n",
    "    embedding_sentence_array = model.encode(sentence_array, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    # Using cosine scores to calculate\n",
    "    cosine_scores = util.pytorch_cos_sim(embedding_right_answer, embedding_sentence_array)\n",
    "    \n",
    "    sorted_indices = cosine_scores.argsort(descending=True)[0]\n",
    "    sorted_array = [sentence_array[i] for i in sorted_indices]\n",
    "    \n",
    "    return sorted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a9f59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# removing value with hash.\n",
    "# Because, from label-tagging before\n",
    "# Some data have a hash symbol, because\n",
    "# that data was part of a word fragment\n",
    "\n",
    "def remove_values_with_hash(arr):\n",
    "    return [item for item in arr if \"#\" not in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf6c62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve stopword from all language\n",
    "\n",
    "response = requests.get(URL_STOPWORD)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    stopword_data = response.json()\n",
    "else:\n",
    "    print(\"Failed to download JSON.\")\n",
    "\n",
    "stopword_data = set([item for sublist in list(stopword_data.values()) for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c043ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function just retrieve random word\n",
    "# of entire premise\n",
    "\n",
    "def select_random_word(text, answer, stopword_data=stopword_data):\n",
    "\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    # Filtering to remove stopword and punctuation\n",
    "    filtered_words = [word for word in words if word not in stopword_data and word not in string.punctuation]\n",
    "    \n",
    "    # If filtered words less than answer\n",
    "    # only take one word as random word\n",
    "    \n",
    "    if len(filtered_words) < len(answer.split()):\n",
    "        random_word = random.choice(filtered_words)\n",
    "    \n",
    "    # But, if filtered words NOT less than answer\n",
    "    # take a same length word as a random word\n",
    "    # with the same order as filtered words\n",
    "    \n",
    "    else:\n",
    "        start_index = random.randint(0, len(filtered_words) - len(answer.split()))\n",
    "        random_word_array = filtered_words[start_index : start_index + len(answer.split())]\n",
    "        random_word = ' '.join(random_word_array)\n",
    "    \n",
    "    return random_word.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d682a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for find the same order\n",
    "# of sequence, this function will used in\n",
    "# chunking domain, to classify whether an\n",
    "# answer is word or a sentence\n",
    "\n",
    "def find_order(premise, answer):\n",
    "    \n",
    "    results = []\n",
    "    answer_labels = [item[0] for item in answer]\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(premise):\n",
    "        if premise[i][0] == answer_labels[0]:\n",
    "            j = 0\n",
    "            matching_words = []\n",
    "            \n",
    "            while i + j < len(premise) and j < len(answer_labels) and premise[i + j][0] == answer_labels[j]:\n",
    "                matching_words.append(premise[i + j][1])\n",
    "                j += 1\n",
    "            \n",
    "            if j == len(answer_labels):\n",
    "                results.append(\" \".join(matching_words))\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76d0afd1-e751-4cb8-ae22-1b7bb10a5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for grouping same tag-label \n",
    "# between answer and word (or sentence) in an entire premise\n",
    "\n",
    "def grouping_same_tag(tag_answers, tag_premises, same_tag_array, tag):\n",
    "    \n",
    "    # In NER, basicly you need to iterate\n",
    "    # to find a same tag-label\n",
    "    \n",
    "    if tag == \"ner\":\n",
    "        for tag_premise in tag_premises:\n",
    "\n",
    "            label_tag_premise = tag_premise[0]\n",
    "            word_premise = tag_premise[1]\n",
    "\n",
    "            for tag_answer in tag_answers:\n",
    "\n",
    "                label_tag_answer = tag_answer[0]\n",
    "\n",
    "                if label_tag_answer == label_tag_premise:\n",
    "                    same_tag_array.append(word_premise)\n",
    "                    \n",
    "    # In Chunking, slightly different\n",
    "    # you need to find subset for find the same order\n",
    "    # of sequence with find_order() function\n",
    "    \n",
    "    elif tag == \"chunking\":\n",
    "        \n",
    "        matching_words = find_order(tag_premises, tag_answers)\n",
    "        \n",
    "        # If there is a correct order of subset answer to premise, add it word\n",
    "        \n",
    "        if len(matching_words) != 0:\n",
    "            for word in matching_words:\n",
    "                same_tag_array.append(word)\n",
    "        \n",
    "        # If no matching words, use NER algorithm above\n",
    "        \n",
    "        else:\n",
    "            grouping_same_tag(tag_answers, tag_premises, same_tag_array, \"ner\")\n",
    "\n",
    "    # Still, filter value with hash\n",
    "    \n",
    "    return remove_values_with_hash(same_tag_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "598b3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# checking text if only\n",
    "# contain punctuation, no words at all \n",
    "\n",
    "def contains_only_punctuation(text):\n",
    "    return all(char in string.punctuation for char in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c20d3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# filter overlapping right answer and wrong answer\n",
    "# that provided in plausible answer\n",
    "\n",
    "def filtering_plausible_answer(answer, plausible_answer_array):\n",
    "    \n",
    "    if type(plausible_answer_array) == str: \n",
    "        plausible_answer_array = list(plausible_answer_array)\n",
    "    \n",
    "    answer = answer.lower()\n",
    "    \n",
    "    plausible_answer_array = [item.lower().strip() for item in plausible_answer_array]\n",
    "    plausible_answer_array = [string for string in plausible_answer_array if not contains_only_punctuation(string)]\n",
    "    plausible_answer_array = [remove_punctuation(text) for text in plausible_answer_array]\n",
    "    \n",
    "    final_plausible_answer_array = list()\n",
    "    answer_words = set(remove_punctuation(text) for text in answer.split())\n",
    "    \n",
    "    # For check overlapping answer, using set of word,\n",
    "    # and so, check for intersection\n",
    "    \n",
    "    for plausible_answer in plausible_answer_array:\n",
    "        plausible_answer_words = set(plausible_answer.split())\n",
    "        if not plausible_answer_words.intersection(answer_words):\n",
    "            final_plausible_answer_array.append(plausible_answer)\n",
    "    \n",
    "    return final_plausible_answer_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddb26145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# detecting number, date, time\n",
    "# to give plausible answer more\n",
    "# \"make sense\" answer\n",
    "\n",
    "def is_number(input_str):\n",
    "    pattern = r'^-?\\d+(\\.\\d+)?$'\n",
    "    return re.match(pattern, input_str) is not None\n",
    "\n",
    "def is_date(input_str):\n",
    "    pattern_1 = r'\\d{1,2} [A-Za-z]+(?: \\d{4})?'\n",
    "    pattern_2 = r'^\\d{4}-\\d{2}-\\d{2}$'\n",
    "    return (re.match(pattern_1, input_str) or re.match(pattern_2, input_str)) is not None\n",
    "\n",
    "def is_time(input_str):\n",
    "    pattern = r'^\\d{2}:\\d{2}:\\d{2}$'\n",
    "    return re.match(pattern, input_str) is not None\n",
    "\n",
    "def check_regex(right_answer, plausible_answer_array):\n",
    "    \n",
    "    if is_number(right_answer):\n",
    "        plausible_answer_array = [item for item in plausible_answer_array if is_number(item)]\n",
    "    \n",
    "    elif is_date(right_answer):\n",
    "        plausible_answer_array = [item for item in plausible_answer_array if is_date(item)]\n",
    "    \n",
    "    elif is_time(right_answer):\n",
    "        plausible_answer_array = [item for item in plausible_answer_array if is_time(item)]\n",
    "        \n",
    "    else:\n",
    "        plausible_answer_array = [item for item in plausible_answer_array if (not is_number(item) or \n",
    "                                                                              not is_date(item) or \n",
    "                                                                              not is_time(item)\n",
    "                                                                             )]\n",
    "    \n",
    "    return plausible_answer_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5369eced-0cee-4d0d-a436-89a97d2af242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# sorting similarity and\n",
    "# to give final wrong answer \n",
    "# and properties\n",
    "\n",
    "def sorting_similarity(data, right_answer, index, tag, plausible_answer_array, premise,\n",
    "                       NO_ANSWER_STATEMENT=NO_ANSWER_STATEMENT):\n",
    "\n",
    "    if tag == \"ner\": slice = 'same_ner_tag_answer'\n",
    "    elif tag == \"chunking\": slice = 'same_chunking_tag_answer'\n",
    "    else: slice = None\n",
    "\n",
    "    # Find all the sorted (by similarity) plausible wrong answer, \n",
    "    # and remove hask & punctuation only answer\n",
    "    \n",
    "    if slice != None:\n",
    "        wrong_answer_array = return_similarity_sorted_array(right_answer, data[slice][index])\n",
    "    \n",
    "    else:\n",
    "        wrong_answer_array = return_similarity_sorted_array(right_answer, plausible_answer_array)\n",
    "    \n",
    "    # Below, do the filtering to plausible answer\n",
    "    \n",
    "    plausible_answer_array = remove_values_with_hash(wrong_answer_array)\n",
    "    plausible_answer_array = filtering_plausible_answer(right_answer, plausible_answer_array)\n",
    "    plausible_answer_array = check_regex(right_answer, plausible_answer_array)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Only return the most similar to right_answer\n",
    "        wrong_answer = plausible_answer_array[0].strip()\n",
    "        \n",
    "        if tag == \"ner\": \n",
    "            properties = \"IDENTICAL NER labels were found, and the highest similarity score same NER array was selected\"\n",
    "        \n",
    "        elif tag == \"chunking\":\n",
    "            properties = \"IDENTICAL Chunking labels were found, and the highest similarity score from same Chunking array was selected\"\n",
    "        \n",
    "        else:\n",
    "            properties = \"NO CHUNKING labels were found, and the highest similarity score from plausible answer was selected\"\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        # Selecting wrong answer from random word in premise\n",
    "        wrong_answer = select_random_word(premise, right_answer)\n",
    "        \n",
    "        # If that random word is overlapping to right answer,\n",
    "        # iterate again until it is not overlap again\n",
    "        \n",
    "        while filtering_plausible_answer(right_answer, wrong_answer) == list():\n",
    "            wrong_answer = select_random_word(premise, right_answer)[0]\n",
    "            break\n",
    "        \n",
    "        # If it still detect overlapped right answer,\n",
    "        # just assign it with NO_ANSWER_STATEMENT.\n",
    "        # But, this condition is very-extraordinary\n",
    "        \n",
    "        else:\n",
    "            wrong_answer = NO_ANSWER_STATEMENT\n",
    "        \n",
    "        if tag == \"ner\": \n",
    "            properties = \"Detected (NER) wrong answer that is the SAME as the right answer, search random word from premise\"\n",
    "        \n",
    "        elif tag == \"chunking\":\n",
    "            properties = \"Detected (Chunking) wrong answer that is the SAME as the right answer, search random word from premise\"\n",
    "        \n",
    "        else:\n",
    "            properties = \"Detected (Random) wrong answer that is the SAME as the right answer, search random word from premise\"\n",
    "    \n",
    "    # Still need to check/assert the wrong answer\n",
    "    # and the plausible answer type\n",
    "    \n",
    "    assert isinstance(wrong_answer, str)\n",
    "    assert isinstance(plausible_answer_array, list)\n",
    "    \n",
    "    return wrong_answer, plausible_answer_array, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97759144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is the main idea to create wrong answer\n",
    "# Though, this function is helper function\n",
    "\n",
    "def create_wrong_answer(data, NO_ANSWER_STATEMENT=NO_ANSWER_STATEMENT):\n",
    "    \n",
    "    data['same_ner_tag_answer'] = \"\"\n",
    "    data['same_chunking_tag_answer'] = \"\"\n",
    "    data['wrong_answer'] = \"\"\n",
    "    data['no_answer'] = \"\"\n",
    "    data['plausible_answer_based_on_method'] = \"\"\n",
    "    data['properties'] = \"\"\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        \n",
    "        right_answer = data['answer'][i]\n",
    "        premise = data['premise'][i]\n",
    "\n",
    "        same_ner_tag_answer_array = list()\n",
    "        same_chunking_tag_answer_array = list()\n",
    "\n",
    "        ner_tag_answer = data['ner_tag_answer'][i]\n",
    "        ner_tag_premise = data['ner_tag_premise'][i]\n",
    "\n",
    "        chunking_tag_answer = data['chunking_tag_answer'][i]\n",
    "        chunking_tag_premise = data['chunking_tag_premise'][i]\n",
    "        \n",
    "        # If that row of data is unanswerable, do this, then continue\n",
    "        \n",
    "        if right_answer == \"\":\n",
    "            data['properties'][i] = \"Unanswerable question\"\n",
    "            data['wrong_answer'][i] = \"NULL\"\n",
    "            data['no_answer'][i] = \"NULL\"\n",
    "            data['plausible_answer_based_on_method'][i] = \"Unanswerable question\"\n",
    "            continue\n",
    "            \n",
    "        # Grouped with the same NER & Chunking group, between answer and word of premise\n",
    "        \n",
    "        data['same_ner_tag_answer'][i] = grouping_same_tag(ner_tag_answer,\n",
    "                                                           ner_tag_premise,\n",
    "                                                           same_ner_tag_answer_array, \"ner\")\n",
    "        \n",
    "        data['same_chunking_tag_answer'][i] = grouping_same_tag(chunking_tag_answer, \n",
    "                                                                chunking_tag_premise, \n",
    "                                                                same_chunking_tag_answer_array, \"chunking\")\n",
    "        \n",
    "        # Start to create wrong answer\n",
    "        plausible_answer_array = list()\n",
    "\n",
    "        # Perform NER classification\n",
    "        # If the NER of the right_answer can be detected, then calculate the distance using semantic \n",
    "        # similarity or word vectors between the right_answer and various possible wrong_answers with \n",
    "        # the same NER as the right_answer. Once done, proceed to the final wrong_answer.\n",
    "        \n",
    "        if data['same_ner_tag_answer'][i] != list():\n",
    "            wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                      i, \"ner\", plausible_answer_array, premise)\n",
    "            \n",
    "        # If the NER of the right_answer cannot be detected (NULL) or context/premise does not contain \n",
    "        # any of NER of right_answer, then the POS/Chunking of the right_answer will be identified.\n",
    "        \n",
    "        # Perform POS/Chunking classification\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # If the POS/Chunking of the right_answer can be detected, then calculate the distance \n",
    "            # using semantic similarity or word vectors between the right_answer and various possible \n",
    "            # wrong_answers with the same POS/Chunking as the right_answer. Once done, proceed to the \n",
    "            # final wrong_answer.\n",
    "            \n",
    "            if data['same_chunking_tag_answer'][i] != list():\n",
    "                wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                          i, \"chunking\", plausible_answer_array, premise)\n",
    "            \n",
    "            # If the POS/Chunking of the right_answer cannot be detected (NULL) or context/premise \n",
    "            # does not contain any of NER of right_answer, then the final wrong_answer will be chosen \n",
    "            # based on a plausible answer.\n",
    "            \n",
    "            else:\n",
    "                for chunking_tag in chunking_tag_premise:\n",
    "                    plausible_answer_array.append(chunking_tag[1])\n",
    "\n",
    "                wrong_answer, plausible_answer_array, properties = sorting_similarity(data, right_answer, \\\n",
    "                                                                          i, \"none\", plausible_answer_array, premise)\n",
    "        data['properties'][i] = properties\n",
    "        data['wrong_answer'][i] = wrong_answer\n",
    "        data['no_answer'][i] = NO_ANSWER_STATEMENT\n",
    "        data['plausible_answer_based_on_method'][i] = plausible_answer_array\n",
    "            \n",
    "    return data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efed3dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d104fab3934c41973706b484859f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90a0f13b2ab40c4a4f7deed313356aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                                | 1/25 [00:01<00:47,  1.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606902e1ea4a4adeb2f4ba50c04e5972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc0ef0490264704bf8da1af09153c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a387c5c2db4242ddb08a9e1304697161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c619506bdc1d4738a7cef98f28f761db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████▍                                                                      | 4/25 [00:02<00:08,  2.49it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ca1b5ac724434d9b6b2af2f9548c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3aace104af7443992375c3b79faf068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c7966725754ee78f3f2b8c508c33f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2440e3e6ab3247908455c3ba5ac16b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████▏                                                               | 6/25 [00:02<00:04,  4.01it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b322b2d2e334b54a9399734ba219934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464b4610150448f1a3f86288c5352b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f637a32c497b4015843a61a1503cc515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9980a197d94b05b6bbf6a10b44ef3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▉                                                         | 8/25 [00:02<00:03,  4.99it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe7bd33c66542b88df042ae61d8f4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aba26759364406086857a52ba7b250a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38737dd6aef2466a905a1bbe0fb8e163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778a9afd1f954c6d9f28e6a8d6e65d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 15/25 [00:02<00:00, 12.68it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb61712b989f48d49dc60889cabbf62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b08c90c5eb04810a54fefe4f5e18a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a70b310bb394d859acd8c00f2e863f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd4166791474e4ead622eb286e21840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████████          | 22/25 [00:02<00:00, 20.76it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3788c153a60404b93bc44ae2fb24d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965a0e9d8a4e41d99e718ac713299d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640a09c7ec184035bd5d9e1338e95b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f705bb784c4d1ba0b8fde66691dfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4ca8ab1cef4f83b6f43f6d979503a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db183ba7f48f46f1adafefa2a69d1805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00,  8.94it/s]\n",
      "  0%|                                                                                            | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88e71522fe64d6fa7ca6dac7b586c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f7a0ff807d43cbac61ec0f845ae153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6bff387386497abe71d1dc0e1708d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8150f6d4d34faa912c3bc10eaaa6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▋                                                                             | 2/25 [00:00<00:01, 16.31it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03470005e1b4cb4821b93978ed9ac29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b28a51a77341efb7e3c018abfe7cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316a813df5564c94ab2c8a679b32ce56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8922f13a4945473a9d5a5cf82d48f9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████▍                                                                      | 4/25 [00:00<00:01, 17.90it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84178d99bb9481db3062b50a77d2211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea94d9d31c3a42039c95d1dd6eb3b589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c846066319534449a48b5bce41839803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986d80ec43b04546b41f8edc6d08f4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▉                                                         | 8/25 [00:00<00:00, 26.16it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5432f8172cb942edbf503572d6745721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c611d7b5c91e4a41bbca759a15f5ce83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f804cbdabe0e40b8b05907c3153624ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cb7fc37dfb4ce386a4abd8deb5657f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████▌                                              | 11/25 [00:00<00:00, 25.30it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aee30ad7aed4b1989a89b813a845ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970d2a41336a45199845e9cf2869e0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bc4fd45236473f80392e7dbd32caae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3ce430516a4f3bb949c1b910878b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████▍                                    | 14/25 [00:00<00:00, 26.18it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fe6d6a1dab461c96275f375f666d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8afa2680274711894674a246617de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa219e327c924df8bba0d70e26f992fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f9a554f0e543b592ddb0e9622adf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████▊                       | 18/25 [00:00<00:00, 23.62it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245bfc86ee51473986a3b035c8db7469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509fc39b81404bf2a39eb635cc4dc667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7c54408c80444ba8d24fe1801b0522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d8f91a99754407a16e87574ceb9634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████████████████████▎      | 23/25 [00:00<00:00, 29.32it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262d84d40d544d2d9eb96c6367a1a81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b657b4b7594a46659cd0ca8bded2c5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 26.80it/s]\n",
      "  0%|                                                                                            | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1bcad51ecc470a9618cdc7b623cce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23498668a2e74d6aa02900017b92e318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53b20321fc044669f2afc92bc371fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824c004fb4b1473b8bf15aa4e63f8480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████                                                                          | 3/25 [00:00<00:01, 21.18it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10c2fd62ccb4f8e828ae787796cd880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca96efa84324dc7827ac4807bc533ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfdf41f70ab45f5a48ed4adbf0b9cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34dff99a56840d3a6a73132622d8f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████▏                                                               | 6/25 [00:00<00:00, 24.64it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9fa9790b6a427e83b78d8637192091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fbffc7a1e2437b898fab0556a267fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 10/25 [00:00<00:00, 27.38it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddecd1443eb640d1a0abe3995750cdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa49643e49145bd818a6bd43e7437c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c57392b3804f5aaf6180dc9b194967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a29af47b7c46dba1caf522e64450f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████▏                                       | 13/25 [00:00<00:00, 26.36it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7ea73a2b374bb397b50390edbe5617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96acd2b38a2481296c9fba5e75c81d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effbfd04a5ea4485939a4dad041ca04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141d6271cfe64a1197a39287dfcbdb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████▍                          | 17/25 [00:00<00:00, 29.97it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34067a9ee384670ad1293bd27a0c58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c97c315cd44d77b5be68f5a59b4ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934e4c7a8e26496caae5f42fba34aaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9632e064f5e44f7ab1b4515c6e9ce442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 34.75it/s]\n"
     ]
    }
   ],
   "source": [
    "data_nli_train_df = create_wrong_answer(data_nli_train_df)\n",
    "data_nli_val_df = create_wrong_answer(data_nli_val_df)\n",
    "data_nli_test_df = create_wrong_answer(data_nli_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe61c82",
   "metadata": {},
   "source": [
    "# Split to two dataset: right dataset & wrong dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9d6c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is just only\n",
    "# for aesthetics of column number\n",
    "\n",
    "def move_to_column_number(data, column_name=\"hypothesis\", column_num=3):\n",
    "\n",
    "    cols = list(data.columns)\n",
    "    cols.remove(column_name)\n",
    "    cols.insert(column_num, column_name)\n",
    "\n",
    "    data = data[cols]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56880d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating answerable right (entailment label) dataset\n",
    "\n",
    "columns_to_exclude = ['wrong_answer', 'no_answer']\n",
    "\n",
    "data_nli_answerable_right_train_df = data_nli_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_answerable_right_val_df = data_nli_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_answerable_right_test_df = data_nli_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_answerable_right_train_df = data_nli_answerable_right_train_df[data_nli_answerable_right_train_df['answer'] != '']\n",
    "data_nli_answerable_right_val_df = data_nli_answerable_right_val_df[data_nli_answerable_right_val_df['answer'] != '']\n",
    "data_nli_answerable_right_test_df = data_nli_answerable_right_test_df[data_nli_answerable_right_test_df['answer'] != '']\n",
    "\n",
    "data_nli_answerable_right_train_df = data_nli_answerable_right_train_df.reset_index(drop=True)\n",
    "data_nli_answerable_right_val_df = data_nli_answerable_right_val_df.reset_index(drop=True)\n",
    "data_nli_answerable_right_test_df = data_nli_answerable_right_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "232c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating answerable wrong (contradiction label) dataset\n",
    "\n",
    "columns_to_exclude = ['answer', 'no_answer']\n",
    "\n",
    "data_nli_answerable_wrong_train_df = data_nli_train_df[data_nli_train_df['answer'] != '']\n",
    "data_nli_answerable_wrong_val_df = data_nli_val_df[data_nli_val_df['answer'] != '']\n",
    "data_nli_answerable_wrong_test_df = data_nli_test_df[data_nli_test_df['answer'] != '']\n",
    "\n",
    "data_nli_answerable_wrong_train_df = data_nli_answerable_wrong_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_answerable_wrong_val_df = data_nli_answerable_wrong_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_answerable_wrong_test_df = data_nli_answerable_wrong_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_answerable_wrong_train_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "data_nli_answerable_wrong_val_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "data_nli_answerable_wrong_test_df.rename(columns={'wrong_answer': 'answer'}, inplace=True)\n",
    "\n",
    "data_nli_answerable_wrong_train_df = data_nli_answerable_wrong_train_df.reset_index(drop=True)\n",
    "data_nli_answerable_wrong_val_df = data_nli_answerable_wrong_val_df.reset_index(drop=True)\n",
    "data_nli_answerable_wrong_test_df = data_nli_answerable_wrong_test_df.reset_index(drop=True)\n",
    "\n",
    "data_nli_answerable_wrong_train_df = move_to_column_number(data_nli_answerable_wrong_train_df, \"answer\", 2)\n",
    "data_nli_answerable_wrong_val_df = move_to_column_number(data_nli_answerable_wrong_val_df, \"answer\", 2)\n",
    "data_nli_answerable_wrong_test_df = move_to_column_number(data_nli_answerable_wrong_test_df, \"answer\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "297a7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unanswerable right (entailment label) and no-answer dataset\n",
    "\n",
    "columns_to_exclude = ['wrong_answer', 'no_answer']\n",
    "\n",
    "data_nli_unanswerable_right_train_df = data_nli_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_unanswerable_right_val_df = data_nli_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_unanswerable_right_test_df = data_nli_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_unanswerable_right_train_df = data_nli_unanswerable_right_train_df[data_nli_unanswerable_right_train_df['answer'] == '']\n",
    "data_nli_unanswerable_right_val_df = data_nli_unanswerable_right_val_df[data_nli_unanswerable_right_val_df['answer'] == '']\n",
    "data_nli_unanswerable_right_test_df = data_nli_unanswerable_right_test_df[data_nli_unanswerable_right_test_df['answer'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef5df327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unanswerable wrong (contradiction label) and no-answer dataset\n",
    "\n",
    "columns_to_exclude = ['answer', 'wrong_answer']\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = data_nli_train_df[data_nli_train_df['answer'] != '']\n",
    "data_nli_unanswerable_wrong_val_df = data_nli_val_df[data_nli_val_df['answer'] != '']\n",
    "data_nli_unanswerable_wrong_test_df = data_nli_test_df[data_nli_test_df['answer'] != '']\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = data_nli_unanswerable_wrong_train_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_unanswerable_wrong_val_df = data_nli_unanswerable_wrong_val_df.drop(columns=columns_to_exclude).copy()\n",
    "data_nli_unanswerable_wrong_test_df = data_nli_unanswerable_wrong_test_df.drop(columns=columns_to_exclude).copy()\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df.rename(columns={'no_answer': 'answer'}, inplace=True)\n",
    "data_nli_unanswerable_wrong_val_df.rename(columns={'no_answer': 'answer'}, inplace=True)\n",
    "data_nli_unanswerable_wrong_test_df.rename(columns={'no_answer': 'answer'}, inplace=True)\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = move_to_column_number(data_nli_unanswerable_wrong_train_df, \"answer\", 2)\n",
    "data_nli_unanswerable_wrong_val_df = move_to_column_number(data_nli_unanswerable_wrong_val_df, \"answer\", 2)\n",
    "data_nli_unanswerable_wrong_test_df = move_to_column_number(data_nli_unanswerable_wrong_test_df, \"answer\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54b4d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rather than duplicating the no-answer statement, \n",
    "# it's better to remove the excessing row ones.\n",
    "\n",
    "def balancing_data(data1, data2):\n",
    "    \n",
    "    if len(data1) > len(data2):\n",
    "        data1 = data1.sample(n=len(data2))\n",
    "    \n",
    "    elif len(data1) < len(data2):\n",
    "        data2 = data2.sample(n=len(data1))\n",
    "        \n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "312fdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_right_train_df, data_nli_unanswerable_wrong_train_df = balancing_data(data_nli_unanswerable_right_train_df,\n",
    "                                                                                            data_nli_unanswerable_wrong_train_df)\n",
    "\n",
    "data_nli_unanswerable_right_val_df, data_nli_unanswerable_wrong_val_df = balancing_data(data_nli_unanswerable_right_val_df,\n",
    "                                                                                        data_nli_unanswerable_wrong_val_df)\n",
    "\n",
    "data_nli_unanswerable_right_test_df, data_nli_unanswerable_wrong_test_df = balancing_data(data_nli_unanswerable_right_test_df,\n",
    "                                                                                        data_nli_unanswerable_wrong_test_df)\n",
    "\n",
    "# Still need to reset index of DataFrame\n",
    "\n",
    "data_nli_unanswerable_right_train_df = data_nli_unanswerable_right_train_df.reset_index(drop=True)\n",
    "data_nli_unanswerable_right_val_df = data_nli_unanswerable_right_val_df.reset_index(drop=True)\n",
    "data_nli_unanswerable_right_test_df = data_nli_unanswerable_right_test_df.reset_index(drop=True)\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = data_nli_unanswerable_wrong_train_df.reset_index(drop=True)\n",
    "data_nli_unanswerable_wrong_val_df = data_nli_unanswerable_wrong_val_df.reset_index(drop=True)\n",
    "data_nli_unanswerable_wrong_test_df = data_nli_unanswerable_wrong_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d4f9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTAILMENT ANSWERABLE\n",
      "TRAIN: 14\n",
      "VAL: 15\n",
      "TEST: 11\n",
      "\n",
      "CONTRADICTION ANSWERABLE\n",
      "TRAIN: 14\n",
      "VAL: 15\n",
      "TEST: 11\n",
      "\n",
      "ENTAILMENT UN-ANSWERABLE\n",
      "TRAIN: 11\n",
      "VAL: 10\n",
      "TEST: 11\n",
      "\n",
      "CONTRADICTION UN-ANSWERABLE\n",
      "TRAIN: 11\n",
      "VAL: 10\n",
      "TEST: 11\n"
     ]
    }
   ],
   "source": [
    "# For debug purpose\n",
    "\n",
    "print(\"ENTAILMENT ANSWERABLE\")\n",
    "print(\"TRAIN:\", len(data_nli_answerable_right_train_df))\n",
    "print(\"VAL:\", len(data_nli_answerable_right_val_df))\n",
    "print(\"TEST:\", len(data_nli_answerable_right_test_df))\n",
    "print()\n",
    "\n",
    "print(\"CONTRADICTION ANSWERABLE\")\n",
    "print(\"TRAIN:\", len(data_nli_answerable_wrong_train_df))\n",
    "print(\"VAL:\", len(data_nli_answerable_wrong_val_df))\n",
    "print(\"TEST:\", len(data_nli_answerable_wrong_test_df))\n",
    "print()\n",
    "\n",
    "print(\"ENTAILMENT UN-ANSWERABLE\")\n",
    "print(\"TRAIN:\", len(data_nli_unanswerable_right_train_df))\n",
    "print(\"VAL:\", len(data_nli_unanswerable_right_val_df))\n",
    "print(\"TEST:\", len(data_nli_unanswerable_right_test_df))\n",
    "print()\n",
    "\n",
    "print(\"CONTRADICTION UN-ANSWERABLE\")\n",
    "print(\"TRAIN:\", len(data_nli_unanswerable_wrong_train_df))\n",
    "print(\"VAL:\", len(data_nli_unanswerable_wrong_val_df))\n",
    "print(\"TEST:\", len(data_nli_unanswerable_wrong_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e6f08",
   "metadata": {},
   "source": [
    "# Convert question-answer pair to hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9cb9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe we can try this approach\n",
    "\n",
    "#nlp_tools_paraphraser = pipeline(task = TASK_PARAPHRASER_NAME, \n",
    "#                     model = MODEL_PARAPHRASER_NAME, \n",
    "#                     tokenizer = AutoTokenizer.from_pretrained(MODEL_PARAPHRASER_NAME, \n",
    "#                                                               model_max_length=512, \n",
    "#                                                               truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01d0b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function useful for\n",
    "# retrieve hypothesis from\n",
    "# question and answer\n",
    "\n",
    "def convert_question_and_answer_to_hypothesis(data, NO_ANSWER_STATEMENT=NO_ANSWER_STATEMENT):\n",
    "    \n",
    "    data['hypothesis'] = \"\"\n",
    "    hypothesis_array = list()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if data['answer'][i] != \"\" and type(data['answer'][i]) == str:\n",
    "            hypothesis_array.append(data['question'][i] + ' ' + data['answer'][i])\n",
    "        else:\n",
    "            hypothesis_array.append(data['question'][i] + ' ' + NO_ANSWER_STATEMENT)\n",
    "        \n",
    "        # Use this to decline no-answer-warning properties\n",
    "        #hypothesis_array.append(data['question'][i] + ' ' + data['answer'][i])\n",
    "        \n",
    "        # Use this to use paraphraser\n",
    "        #hypothesis_array.append(str(nlp_tools_paraphraser(data['question'][i] + ' ' + data['answer'][i])[0]['generated_text']))\n",
    "    \n",
    "    data['hypothesis'] = hypothesis_array\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acf340e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_answerable_right_train_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_right_train_df)\n",
    "data_nli_answerable_right_val_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_right_val_df)\n",
    "data_nli_answerable_right_test_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_right_test_df)\n",
    "\n",
    "data_nli_answerable_right_train_df = move_to_column_number(data_nli_answerable_right_train_df, \"hypothesis\", 3)\n",
    "data_nli_answerable_right_val_df = move_to_column_number(data_nli_answerable_right_val_df, \"hypothesis\", 3)\n",
    "data_nli_answerable_right_test_df = move_to_column_number(data_nli_answerable_right_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20241aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_answerable_wrong_train_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_wrong_train_df)\n",
    "data_nli_answerable_wrong_val_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_wrong_val_df)\n",
    "data_nli_answerable_wrong_test_df = convert_question_and_answer_to_hypothesis(data_nli_answerable_wrong_test_df)\n",
    "\n",
    "data_nli_answerable_wrong_train_df = move_to_column_number(data_nli_answerable_wrong_train_df, \"hypothesis\", 3)\n",
    "data_nli_answerable_wrong_val_df = move_to_column_number(data_nli_answerable_wrong_val_df, \"hypothesis\", 3)\n",
    "data_nli_answerable_wrong_test_df = move_to_column_number(data_nli_answerable_wrong_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8706a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_right_train_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_right_train_df)\n",
    "data_nli_unanswerable_right_val_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_right_val_df)\n",
    "data_nli_unanswerable_right_test_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_right_test_df)\n",
    "\n",
    "data_nli_unanswerable_right_train_df = move_to_column_number(data_nli_unanswerable_right_train_df, \"hypothesis\", 3)\n",
    "data_nli_unanswerable_right_val_df = move_to_column_number(data_nli_unanswerable_right_val_df, \"hypothesis\", 3)\n",
    "data_nli_unanswerable_right_test_df = move_to_column_number(data_nli_unanswerable_right_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5d28ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_wrong_train_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_wrong_train_df)\n",
    "data_nli_unanswerable_wrong_val_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_wrong_val_df)\n",
    "data_nli_unanswerable_wrong_test_df = convert_question_and_answer_to_hypothesis(data_nli_unanswerable_wrong_test_df)\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = move_to_column_number(data_nli_unanswerable_wrong_train_df, \"hypothesis\", 3)\n",
    "data_nli_unanswerable_wrong_val_df = move_to_column_number(data_nli_unanswerable_wrong_val_df, \"hypothesis\", 3)\n",
    "data_nli_unanswerable_wrong_test_df = move_to_column_number(data_nli_unanswerable_wrong_test_df, \"hypothesis\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c4ace",
   "metadata": {},
   "source": [
    "# Assign the label: entailment & contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45df14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_answerable_right_train_df['label'] = 'entailment'\n",
    "data_nli_answerable_right_val_df['label'] = 'entailment'\n",
    "data_nli_answerable_right_test_df['label'] = 'entailment'\n",
    "\n",
    "data_nli_answerable_right_train_df = move_to_column_number(data_nli_answerable_right_train_df, \"label\", 4)\n",
    "data_nli_answerable_right_val_df = move_to_column_number(data_nli_answerable_right_val_df, \"label\", 4)\n",
    "data_nli_answerable_right_test_df = move_to_column_number(data_nli_answerable_right_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02098578",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_answerable_wrong_train_df['label'] = 'contradiction'\n",
    "data_nli_answerable_wrong_val_df['label'] = 'contradiction'\n",
    "data_nli_answerable_wrong_test_df['label'] = 'contradiction'\n",
    "\n",
    "data_nli_answerable_wrong_train_df = move_to_column_number(data_nli_answerable_wrong_train_df, \"label\", 4)\n",
    "data_nli_answerable_wrong_val_df = move_to_column_number(data_nli_answerable_wrong_val_df, \"label\", 4)\n",
    "data_nli_answerable_wrong_test_df = move_to_column_number(data_nli_answerable_wrong_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "198ebcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_right_train_df['label'] = 'entailment'\n",
    "data_nli_unanswerable_right_val_df['label'] = 'entailment'\n",
    "data_nli_unanswerable_right_test_df['label'] = 'entailment'\n",
    "\n",
    "data_nli_unanswerable_right_train_df = move_to_column_number(data_nli_unanswerable_right_train_df, \"label\", 4)\n",
    "data_nli_unanswerable_right_val_df = move_to_column_number(data_nli_unanswerable_right_val_df, \"label\", 4)\n",
    "data_nli_unanswerable_right_test_df = move_to_column_number(data_nli_unanswerable_right_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d219b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_unanswerable_wrong_train_df['label'] = 'contradiction'\n",
    "data_nli_unanswerable_wrong_val_df['label'] = 'contradiction'\n",
    "data_nli_unanswerable_wrong_test_df['label'] = 'contradiction'\n",
    "\n",
    "data_nli_unanswerable_wrong_train_df = move_to_column_number(data_nli_unanswerable_wrong_train_df, \"label\", 4)\n",
    "data_nli_unanswerable_wrong_val_df = move_to_column_number(data_nli_unanswerable_wrong_val_df, \"label\", 4)\n",
    "data_nli_unanswerable_wrong_test_df = move_to_column_number(data_nli_unanswerable_wrong_test_df, \"label\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f431144",
   "metadata": {},
   "source": [
    "# Concat the right and wrong NLI to one NLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "165d72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df_final = pd.concat([data_nli_answerable_right_train_df, \n",
    "                                     data_nli_answerable_wrong_train_df,\n",
    "                                     data_nli_unanswerable_right_train_df,\n",
    "                                     data_nli_unanswerable_wrong_train_df], axis=0, ignore_index=True)\n",
    "\n",
    "data_nli_val_df_final = pd.concat([data_nli_answerable_right_val_df, \n",
    "                                   data_nli_answerable_wrong_val_df,\n",
    "                                   data_nli_unanswerable_right_val_df,\n",
    "                                   data_nli_unanswerable_wrong_val_df], axis=0, ignore_index=True)\n",
    "\n",
    "data_nli_test_df_final = pd.concat([data_nli_answerable_right_test_df, \n",
    "                                    data_nli_answerable_wrong_test_df,\n",
    "                                    data_nli_unanswerable_right_test_df,\n",
    "                                    data_nli_unanswerable_wrong_test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a980a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug purpose,\n",
    "# you can modify it too\n",
    "\n",
    "def debug_data(data):\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        print(f\"Iteration: {i}\")\n",
    "        print(f\"Answer: {data['answer'][i]}\")\n",
    "        \n",
    "        print(\"NER\")\n",
    "        print(data['ner_tag_answer'][i])\n",
    "        print(data['ner_tag_premise'][i])\n",
    "        \n",
    "        print(\"Chunking\")\n",
    "        print(data['chunking_tag_answer'][i])\n",
    "        print(data['chunking_tag_premise'][i])\n",
    "        print()\n",
    "\n",
    "# debug_data(data_nli_train_df_final)\n",
    "# debug_data(data_nli_val_df_final)\n",
    "# debug_data(data_nli_test_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2e0b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN FINAL\n",
      "50\n",
      "\n",
      "VAL FINAL\n",
      "50\n",
      "\n",
      "TEST FINAL\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "# For debug purpose\n",
    "\n",
    "print(\"TRAIN FINAL\")\n",
    "print(len(data_nli_train_df_final))\n",
    "print()\n",
    "\n",
    "print(\"VAL FINAL\")\n",
    "print(len(data_nli_val_df_final))\n",
    "print()\n",
    "\n",
    "print(\"TEST FINAL\")\n",
    "print(len(data_nli_test_df_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea9097",
   "metadata": {},
   "source": [
    "# Convert to DataFrame format to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "794bedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nli_train_df_final.to_csv(f\"{NAME}_nli_train_df.csv\", index=False)\n",
    "data_nli_val_df_final.to_csv(f\"{NAME}_nli_val_df.csv\", index=False)\n",
    "data_nli_test_df_final.to_csv(f\"{NAME}_nli_test_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85460bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM FINISHED\n"
     ]
    }
   ],
   "source": [
    "print(\"PROGRAM FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4a255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
