{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c75fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "import string\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29e7d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'xlm-roberta-large'\n",
    "TYPE_QAS = \"entailment_only\"\n",
    "TYPE_SMOOTHING = \"just_concat_answer_and_question\"\n",
    "MAXIMUM_SEARCH_ITER = 3\n",
    "VARIATION = 3\n",
    "THRESHOLD = 0.5\n",
    "MODEL_SC_NAME = \"muhammadravi251001/fine-tuned-NLI-indonli-with-xlm-roberta-large\"\n",
    "\n",
    "USER = \"muhammadravi251001\"   \n",
    "MODEL_TG_IND_NAME = \"Wikidepia/IndoT5-base-paraphrase\"\n",
    "MODEL_TG_ENG_NAME = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "MODEL_NER_NAME = \"ageng-anugrah/indobert-large-p2-finetuned-ner\"\n",
    "MAX_LENGTH = 512\n",
    "STRIDE = 128\n",
    "LOGGING_STEPS = 50\n",
    "WARMUP_RATIO = 0.0\n",
    "WEIGHT_DECAY = 0.0\n",
    "EVAL_STEPS_RATIO = 0.5\n",
    "SAMPLE = sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9a7d44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {'truncation': True, 'max_length': 512}\n",
    "MODEL_QA_NAME = \"muhammadravi251001/fine-tuned-DatasetQAS-IDK-MRC-with-xlm-roberta-large-without-ITTL-without-freeze-LR-1e-05\"\n",
    "\n",
    "nlp_qa = pipeline(task=\"question-answering\", model=MODEL_QA_NAME, tokenizer=MODEL_QA_NAME, \n",
    "                device=torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc8d6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Di Hispania, Ataulf dengan tidak hati-hati menerima pengabdiannya kepada salah satu bekas pengikut almarhum Sarus, tidak menyadari bahwa pria tersebut menyimpan sebuah keinginan rahasia untuk membalas kematian pelindung kesayangannya. Jadi, di istana Barcelona, pria yang membuat kekuasaan Ataulf tiba-tiba berakhir dengan membunuhnya saat dia mandi.\"\n",
    "question = \"Dimana Raja  Ataulf meninggal?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "73d37fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawaban: Hispania, Ataulf dengan tidak hati-hati menerima pengabdiannya kepada salah satu bekas pengikut almarhum Sarus, tidak menyadari bahwa pria tersebut menyimpan sebuah keinginan rahasia untuk membalas kematian pelindung kesayangannya. Jadi, di istana Barcelona\n",
      "Jawaban: \n",
      "Jawaban: Barcelona, pria yang membuat kekuasaan Ataulf tiba-tiba berakhir dengan membunuhnya saat dia mandi\n",
      "Jawaban: \n",
      "Jawaban: di istana\n",
      "Jawaban: \n",
      "Jawaban: \n",
      "Jawaban: \n",
      "Jawaban: \n",
      "Jawaban: \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_QA_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_QA_NAME)\n",
    "model.config.top_k = 10\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "sorted_start_logits = torch.argsort(outputs.start_logits)\n",
    "sorted_end_logits = torch.argsort(outputs.end_logits)\n",
    "\n",
    "for i in range(1, 10+1):\n",
    "    start_index = sorted_start_logits[0, -i]\n",
    "    end_index = sorted_end_logits[0, -i]\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "    print(\"Jawaban:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a4e3e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawaban: istana Barcelona,\n",
      "Jawaban: Hispania,\n",
      "Jawaban: Barcelona,\n",
      "Jawaban: di istana Barcelona,\n",
      "Jawaban: Di Hispania,\n",
      "Jawaban: istana Barcelona,\n",
      "Jawaban: Jadi, di istana Barcelona,\n",
      "Jawaban: saat dia mandi.\n",
      "Jawaban: istana\n",
      "Jawaban: mandi.\n"
     ]
    }
   ],
   "source": [
    "x = nlp_qa(question=question, context=context, top_k=10)\n",
    "\n",
    "for i in x:\n",
    "    print(f\"Jawaban: {i['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a4f08d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawaban: Hispania, Ataulf dengan tidak hati-hati menerima pengabdiannya kepada salah satu bekas pengikut almarhum Sarus, tidak menyadari bahwa pria tersebut menyimpan sebuah keinginan rahasia untuk membalas kematian pelindung kesayangannya. Jadi, di istana Barcelona\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Inisialisasi model dan tokenizer untuk tugas question-answering\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(MODEL_QA_NAME)\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(MODEL_QA_NAME)\n",
    "\n",
    "# Fungsi kustom yang meniru perilaku pipeline\n",
    "def custom_qa(text):\n",
    "    # Tokenisasi input\n",
    "    inputs = tokenizer_qa(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    # Lakukan prediksi menggunakan model question-answering\n",
    "    qa_outputs = model_qa(**inputs)\n",
    "\n",
    "    # Ambil hasil prediksi\n",
    "    start_index = torch.argmax(qa_outputs.start_logits)\n",
    "    end_index = torch.argmax(qa_outputs.end_logits)\n",
    "\n",
    "    # Mendekode token untuk mendapatkan jawaban\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "    answer = tokenizer_qa.decode(answer_tokens)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Contoh penggunaan variabel custom_qa\n",
    "answer = custom_qa(f\"{question} {context}\")\n",
    "print(\"Jawaban:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3def8226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'PLACE', 'score': 1.0, 'index': 4, 'word': 'jakarta', 'start': 4, 'end': 4}, {'entity': 'PERSON', 'score': 1.0, 'index': 9, 'word': 'rafi', 'start': 8, 'end': 9}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Inisialisasi model dan tokenizer untuk tugas NER\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(MODEL_NER_NAME)\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(MODEL_NER_NAME)\n",
    "\n",
    "# Fungsi kustom yang menggunakan .predict()\n",
    "def custom_ner(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        ner_outputs = model(**inputs)\n",
    "\n",
    "    predicted_labels = torch.argmax(ner_outputs.logits, dim=2)[0].tolist()\n",
    "\n",
    "    entity_results = []\n",
    "    entity = None\n",
    "    start = None\n",
    "\n",
    "    for i, label_id in enumerate(predicted_labels):\n",
    "        label = model.config.id2label[label_id]\n",
    "        if label.startswith('B-'):\n",
    "            if entity:\n",
    "                entity_results.append({\n",
    "                    'entity': entity[2:],\n",
    "                    'score': 1.0,\n",
    "                    'index': i - 1,\n",
    "                    'word': tokenizer.decode(inputs['input_ids'][0, start:i].tolist()),\n",
    "                    'start': start,\n",
    "                    'end': i - 1\n",
    "                })\n",
    "            entity = label\n",
    "            start = i\n",
    "        elif label.startswith('I-') and entity:\n",
    "            continue\n",
    "        else:\n",
    "            if entity:\n",
    "                entity_results.append({\n",
    "                    'entity': entity[2:],\n",
    "                    'score': 1.0,\n",
    "                    'index': i - 1,\n",
    "                    'word': tokenizer.decode(inputs['input_ids'][0, start:i].tolist()),\n",
    "                    'start': start,\n",
    "                    'end': i - 1\n",
    "                })\n",
    "                entity = None\n",
    "\n",
    "    return entity_results\n",
    "\n",
    "# Contoh penggunaan variabel custom_ner\n",
    "text = \"Rumah saya di Jakarta, nama saya Rafi\"\n",
    "ner_results = custom_ner(text, tokenizer_ner, model_ner)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "139a1cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PP', 'score': 1.0, 'index': 0, 'word': 'rumah', 'start': 0, 'end': 0}, {'entity': 'B-NP', 'score': 1.0, 'index': 1, 'word': 'saya', 'start': 1, 'end': 1}, {'entity': 'I-NP', 'score': 1.0, 'index': 2, 'word': 'di', 'start': 2, 'end': 2}, {'entity': 'B-PP', 'score': 1.0, 'index': 3, 'word': 'jakarta', 'start': 3, 'end': 3}, {'entity': 'B-NP', 'score': 1.0, 'index': 4, 'word': ',', 'start': 4, 'end': 4}, {'entity': 'O', 'score': 1.0, 'index': 5, 'word': 'nama', 'start': 5, 'end': 5}, {'entity': 'B-NP', 'score': 1.0, 'index': 6, 'word': 'saya', 'start': 6, 'end': 6}, {'entity': 'I-NP', 'score': 1.0, 'index': 7, 'word': 'raf', 'start': 7, 'end': 7}]\n"
     ]
    }
   ],
   "source": [
    "TASK_CHUNKING_NAME = \"token-classification\"\n",
    "MODEL_CHUNKING_NAME = \"ageng-anugrah/indobert-large-p2-finetuned-chunking\"\n",
    "\n",
    "tokenizer_chunking = AutoTokenizer.from_pretrained(MODEL_CHUNKING_NAME)\n",
    "model_chunking = AutoModelForTokenClassification.from_pretrained(MODEL_CHUNKING_NAME)\n",
    "\n",
    "def predict(model, tokenizer, sentence):\n",
    "    \n",
    "    inputs = tokenizer_chunking(sentence,\n",
    "                        return_offsets_mapping=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                        **tokenizer_kwargs)\n",
    "        \n",
    "    ids = inputs[\"input_ids\"]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Proses forward\n",
    "    outputs = model_chunking(ids, attention_mask=mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    active_logits = logits.view(-1, model_chunking.config.num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, dim=1)\n",
    "\n",
    "    tokens = tokenizer_chunking.tokenize(sentence)\n",
    "    token_predictions = [model_chunking.config.id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"].squeeze().tolist()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    entity = None\n",
    "    start_index = 0\n",
    "\n",
    "    for i, (token, token_pred, mapping) in enumerate(zip(tokens, token_predictions, offset_mapping)):\n",
    "        # hanya prediksi pada token pertama yang penting\n",
    "        if entity:\n",
    "            results.append({\n",
    "                'entity': entity,\n",
    "                'score': 1.0,\n",
    "                'index': i - 1,\n",
    "                'word': tokens[start_index:i][0],\n",
    "                'start': start_index,\n",
    "                'end': i - 1\n",
    "            })\n",
    "        entity = token_pred\n",
    "        start_index = i\n",
    "\n",
    "    return results\n",
    "\n",
    "sentence = \"Rumah saya di Jakarta, nama saya Rafi\"\n",
    "\n",
    "predicted_results = predict(model_chunking, tokenizer_chunking, sentence)\n",
    "print(predicted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7ed3a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-NP',\n",
       "  'score': 0.99981314,\n",
       "  'index': 1,\n",
       "  'word': 'rumah',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity': 'I-NP',\n",
       "  'score': 0.9776594,\n",
       "  'index': 2,\n",
       "  'word': 'saya',\n",
       "  'start': 6,\n",
       "  'end': 10},\n",
       " {'entity': 'B-PP',\n",
       "  'score': 0.9982919,\n",
       "  'index': 3,\n",
       "  'word': 'di',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'B-NP',\n",
       "  'score': 0.9987502,\n",
       "  'index': 4,\n",
       "  'word': 'jakarta',\n",
       "  'start': 14,\n",
       "  'end': 21},\n",
       " {'entity': 'B-NP',\n",
       "  'score': 0.9893422,\n",
       "  'index': 6,\n",
       "  'word': 'nama',\n",
       "  'start': 23,\n",
       "  'end': 27},\n",
       " {'entity': 'I-NP',\n",
       "  'score': 0.975521,\n",
       "  'index': 7,\n",
       "  'word': 'saya',\n",
       "  'start': 28,\n",
       "  'end': 32},\n",
       " {'entity': 'I-NP',\n",
       "  'score': 0.70781696,\n",
       "  'index': 8,\n",
       "  'word': 'raf',\n",
       "  'start': 33,\n",
       "  'end': 36},\n",
       " {'entity': 'I-NP',\n",
       "  'score': 0.9916013,\n",
       "  'index': 9,\n",
       "  'word': '##i',\n",
       "  'start': 36,\n",
       "  'end': 37}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_chunking = pipeline(task=\"token-classification\", model=MODEL_CHUNKING_NAME, tokenizer=MODEL_CHUNKING_NAME)\n",
    "\n",
    "chunking = nlp_chunking(\"Rumah saya di Jakarta, nama saya Rafi\")\n",
    "chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d5b3855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "B-PLACE\n",
      "O\n",
      "O\n",
      "O\n",
      "[{'entity': 'B-PLACE', 'score': 1.0, 'index': 4, 'word': ',', 'start': 4, 'end': 4}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_ner = AutoTokenizer.from_pretrained(MODEL_NER_NAME)\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(MODEL_NER_NAME)\n",
    "\n",
    "def predict(model, tokenizer, sentence):\n",
    "    \n",
    "    inputs = tokenizer_ner(sentence,\n",
    "                        return_offsets_mapping=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                        **tokenizer_kwargs)\n",
    "        \n",
    "    ids = inputs[\"input_ids\"]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Proses forward\n",
    "    outputs = model_ner(ids, attention_mask=mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    active_logits = logits.view(-1, model_ner.config.num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, dim=1)\n",
    "\n",
    "    tokens = tokenizer_ner.tokenize(sentence)\n",
    "    token_predictions = [model_ner.config.id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"].squeeze().tolist()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    entity = None\n",
    "    start_index = 0\n",
    "\n",
    "    for i, (token, token_pred, mapping) in enumerate(zip(tokens, token_predictions, offset_mapping)):\n",
    "        # hanya prediksi pada token pertama yang penting\n",
    "        \n",
    "        print(entity)\n",
    "        \n",
    "        if entity and entity != 'O':\n",
    "            results.append({\n",
    "                'entity': entity,\n",
    "                'score': 1.0,\n",
    "                'index': i - 1,\n",
    "                'word': tokens[start_index:i][0],\n",
    "                'start': start_index,\n",
    "                'end': i - 1\n",
    "            })\n",
    "        entity = token_pred\n",
    "        start_index = i\n",
    "\n",
    "    return results\n",
    "\n",
    "sentence = \"Rumah saya di Jakarta, nama saya Rafi\"\n",
    "\n",
    "predicted_results = predict(model_ner, tokenizer_ner, sentence)\n",
    "print(predicted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10019324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PLACE',\n",
       "  'score': 0.9998228,\n",
       "  'index': 4,\n",
       "  'word': 'jakarta',\n",
       "  'start': 14,\n",
       "  'end': 21},\n",
       " {'entity': 'B-PERSON',\n",
       "  'score': 0.9993711,\n",
       "  'index': 8,\n",
       "  'word': 'raf',\n",
       "  'start': 33,\n",
       "  'end': 36},\n",
       " {'entity': 'I-PERSON',\n",
       "  'score': 0.9902058,\n",
       "  'index': 9,\n",
       "  'word': '##i',\n",
       "  'start': 36,\n",
       "  'end': 37}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ner = pipeline(task=\"ner\", model=MODEL_NER_NAME, tokenizer=MODEL_NER_NAME)\n",
    "\n",
    "ner = nlp_ner(\"Rumah saya di Jakarta, nama saya Rafi\")\n",
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae5e43a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "{'label': 'contradiction', 'score': 0.9868278503417969}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "tokenizer_kwargs = {'truncation': True, 'max_length': 512}\n",
    "\n",
    "# Inisialisasi model dan tokenizer untuk tugas text-classification\n",
    "tokenizer_sc = AutoTokenizer.from_pretrained(MODEL_SC_NAME)\n",
    "model_sc = AutoModelForSequenceClassification.from_pretrained(MODEL_SC_NAME)\n",
    "\n",
    "# Fungsi kustom yang meniru perilaku pipeline\n",
    "def custom_text_classification(text_dict):\n",
    "    \n",
    "    inputs = tokenizer_sc(text_dict['text'], text_dict['text_pair'], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model_sc(**inputs)\n",
    "\n",
    "    label_id = torch.argmax(outputs.logits).item()\n",
    "    label = model_sc.config.id2label[label_id]\n",
    "    score = outputs.logits.softmax(dim=-1)[0][label_id].item()\n",
    "\n",
    "    return {'label': label, 'score': score}\n",
    "\n",
    "# Contoh penggunaan variabel custom_text_classification\n",
    "context_decoded = \"Bambang Pamungkas adalah pemain sepak bola asal Bandung\"\n",
    "pred_hypothesis = \"Bambang Pamungkas asal Jakarta\"\n",
    "\n",
    "# Memanggil fungsi custom_text_classification dengan model dan tokenizer yang sesuai\n",
    "predicted_result = custom_text_classification({'text': context_decoded, 'text_pair': pred_hypothesis})\n",
    "\n",
    "print(predicted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3143100e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'Ir. Basuki Tjahaja Purnama'}, {'answer': ''}, {'answer': ''}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_qa(question, context):\n",
    "        \n",
    "    inputs = tokenizer_qa(question, context, \n",
    "                          return_tensors=\"pt\",\n",
    "                          **tokenizer_kwargs)\n",
    "\n",
    "    outputs = model_qa(**inputs)\n",
    "\n",
    "    sorted_start_logits = torch.argsort(outputs.start_logits)\n",
    "    sorted_end_logits = torch.argsort(outputs.end_logits)\n",
    "\n",
    "    answer_array = []\n",
    "    for i in range(1, (MAXIMUM_SEARCH_ITER + 1)):\n",
    "\n",
    "        start_index = sorted_start_logits[0, -i]\n",
    "        end_index = sorted_end_logits[0, -i]\n",
    "        answer_tokens = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "\n",
    "        answer = tokenizer_qa.decode(answer_tokens)\n",
    "        answer_array.append({'answer': answer})\n",
    "\n",
    "    return answer_array\n",
    "\n",
    "custom_qa(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3b150e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.7026851773262024, 'start': 980, 'end': 988, 'answer': '28997m2,'}, {'score': 0.01276418287307024, 'start': 1103, 'end': 1115, 'answer': '29095m2.[16]'}]\n"
     ]
    }
   ],
   "source": [
    "x = nlp_qa(question=question, context=context, top_k=3)\n",
    "\n",
    "unique_start_end = set()\n",
    "unique_answers = []\n",
    "\n",
    "for answer in answers:\n",
    "    \n",
    "    start_end_pair = (answer['start'], answer['end'])\n",
    "\n",
    "    if start_end_pair not in unique_start_end:\n",
    "        unique_start_end.add(start_end_pair)\n",
    "        unique_answers.append(answer)\n",
    "\n",
    "print(unique_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be1a1383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.7026851773262024, 'start': 980, 'end': 988, 'answer': '28997m2,'}, {'score': 0.01276418287307024, 'start': 1103, 'end': 1115, 'answer': '29095m2.[16]'}]\n"
     ]
    }
   ],
   "source": [
    "answers = [{'score': 0.7026851773262024, 'start': 980, 'end': 988, 'answer': '28997m2,'},\n",
    " {'score': 0.01276418287307024, 'start': 1103, 'end': 1115, 'answer': '29095m2.[16]'},\n",
    " {'score': 0.0036276059690862894, 'start': 980, 'end': 988, 'answer': '28997m2,'}]\n",
    "\n",
    "# Buat set kosong untuk menyimpan pasangan start-end unik\n",
    "unique_start_end = set()\n",
    "\n",
    "# Buat daftar baru untuk jawaban unik\n",
    "unique_answers = []\n",
    "\n",
    "for answer in answers:\n",
    "    start_end_pair = (answer['start'], answer['end'])\n",
    "\n",
    "    # Cek apakah pasangan start-end sudah ada dalam set\n",
    "    if start_end_pair not in unique_start_end:\n",
    "        unique_start_end.add(start_end_pair)\n",
    "        unique_answers.append(answer)\n",
    "\n",
    "# unique_answers sekarang berisi jawaban dengan start-end yang unik\n",
    "print(unique_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b92a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Saya kapitan'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_tg_ind = pipeline(task=\"text2text-generation\", model=MODEL_TG_IND_NAME, tokenizer=MODEL_TG_IND_NAME, device=torch.cuda.current_device(), **tokenizer_kwargs)\n",
    "\n",
    "nlp_tg_ind(\"Saya seorang kapitan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5440b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Saya kapitan'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_TG_IND_NAME = \"Wikidepia/IndoT5-base-paraphrase\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_tg_ind = T5ForConditionalGeneration.from_pretrained(MODEL_TG_IND_NAME)\n",
    "tokenizer_tg_ind = T5Tokenizer.from_pretrained(MODEL_TG_IND_NAME)\n",
    "model_tg_ind.to(\"cuda\")  # Pindahkan model ke GPU jika tersedia\n",
    "\n",
    "# Fungsi untuk melakukan generasi teks\n",
    "def nlp_tg_ind(prompt):\n",
    "    input_ids = tokenizer_tg_ind(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    output = model_tg_ind.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "    generated_text = tokenizer_tg_ind.batch_decode(output, skip_special_tokens=True)\n",
    "    \n",
    "    return [{'generated_text': generated_text[0]}]\n",
    "\n",
    "# Menggunakan nlp_tg_ind\n",
    "result = nlp_tg_ind(\"Saya seorang kapitan\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "105cb1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Saya seorang kapitan kapitan kapitaran sabuk terhada.'}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_tg_eng = pipeline(task=\"text2text-generation\", model=MODEL_TG_ENG_NAME, tokenizer=MODEL_TG_ENG_NAME, device=torch.cuda.current_device(), **tokenizer_kwargs)\n",
    "\n",
    "nlp_tg_eng(\"Saya seorang kapitan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9602b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Saya seorang kapitan adilirih sabukh di dua dalah saya salaman terhada.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_TG_ENG_NAME = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_tg_eng = T5ForConditionalGeneration.from_pretrained(MODEL_TG_ENG_NAME)\n",
    "tokenizer_tg_eng = T5Tokenizer.from_pretrained(MODEL_TG_ENG_NAME) # Pindahkan model ke GPU jika tersedia\n",
    "\n",
    "# Fungsi untuk melakukan generasi teks\n",
    "def nlp_tg_eng(prompt):\n",
    "    input_ids = tokenizer_tg_eng(prompt, return_tensors=\"pt\").input_ids\n",
    "    output = model_tg_eng.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "    generated_text = tokenizer_tg_eng.batch_decode(output, skip_special_tokens=True)\n",
    "    \n",
    "    return [{'generated_text': generated_text[0]}]\n",
    "\n",
    "# Menggunakan nlp_tg_ind\n",
    "result = nlp_tg_eng(\"Saya seorang kapitan\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c49ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
