{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a665743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "import string\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6514a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {'truncation': True, 'max_length': 512}\n",
    "MODEL_QA_NAME = \"muhammadravi251001/fine-tuned-DatasetQAS-IDK-MRC-with-xlm-roberta-large-without-ITTL-without-freeze-LR-1e-05\"\n",
    "\n",
    "pipeline_qa = pipeline(task=\"question-answering\", model=MODEL_QA_NAME, tokenizer=MODEL_QA_NAME, \n",
    "                device=torch.cuda.current_device())\n",
    "\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(MODEL_QA_NAME)\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(MODEL_QA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d859797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_qa_uncomment(question, context):\n",
    "    \n",
    "    model_qa.eval()\n",
    "    \n",
    "    inputs = tokenizer_qa(question, context, \n",
    "                          return_tensors=\"pt\",\n",
    "                          **tokenizer_kwargs)\n",
    "    \n",
    "    outputs = model_qa(**inputs)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    #max_start_logit = torch.max(start_logits).item()\n",
    "    #max_end_logit = torch.max(end_logits).item()\n",
    "    #min_max_range = max_end_logit - max_start_logit\n",
    "    \n",
    "    start_index = torch.argmax(start_logits)\n",
    "    end_index = torch.argmax(end_logits)\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "    answer = tokenizer_qa.decode(answer_tokens)\n",
    "\n",
    "    #start_score = (torch.max(start_logits).item() - max_start_logit) / min_max_range\n",
    "    #end_score = (torch.max(end_logits).item() - max_start_logit) / min_max_range\n",
    "    #score = (start_score + end_score) / 2.0\n",
    "    \n",
    "    #return {'score': score, 'start': start_index.item(), 'end': end_index.item(), 'answer': answer}\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f845102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_qa(question, context):\n",
    "    \n",
    "    model_qa.eval()\n",
    "    \n",
    "    inputs = tokenizer_qa(question, context, \n",
    "                          return_tensors=\"pt\",\n",
    "                          **tokenizer_kwargs)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_qa(**inputs)\n",
    "    \n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    start_index = torch.argmax(start_logits)\n",
    "    end_index = torch.argmax(end_logits)\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "    answer = tokenizer_qa.decode(answer_tokens)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6617ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_qa_grad(question, context):\n",
    "    \n",
    "    model_qa.eval()\n",
    "    \n",
    "    inputs = tokenizer_qa(question, context, \n",
    "                          return_tensors=\"pt\",\n",
    "                          **tokenizer_kwargs)\n",
    "    outputs = model_qa(**inputs)\n",
    "    \n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    start_index = torch.argmax(start_logits)\n",
    "    end_index = torch.argmax(end_logits)\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "    answer = tokenizer_qa.decode(answer_tokens)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b973ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context1 = \"Mikail (Arab: ميكائيل) adalah malaikat yang mengatur air, menurunkan hujan/petir, membagikan rezeki pada manusia, tumbuh-tumbuhan juga hewan-hewan dan lain-lain di muka bumi ini. Dikatakan setiap satu makhluk yang memerlukan rezeki untuk hidup di dunia ini akan diselia rezekinya oleh satu malaikat Karubiyyuun.\"\n",
    "question1 = \"Apa tugas Malaikat Mikhael di luar Islam?\"\n",
    "\n",
    "context2 = \"Gagasan atau peluang adalah istilah yang dipakai baik secara populer maupun dalam bidang filsafat dengan pengertian umum \\\"citra mental\\\" atau \\\"pengertian\\\". Terutama Plato adalah eksponen pemikiran seperti ini.\"\n",
    "question2 = \"apakah yang dimaksud dengan gagasan?\"\n",
    "\n",
    "context3 = \"Pornografi dalam rancangan pertama didefinisikan sebagai \\\"substansi dalam media atau alat komunikasi yang dibuat untuk menyampaikan gagasan-gagasan yang mengeksploitasi seksual, kecabulan, dan/atau erotika\\\" sementara pornoaksi adalah \\\"perbuatan mengeksploitasi seksual, kecabulan, dan/atau erotika di muka umum\\\".\"\n",
    "question3 = \"Apa yang dimaksud dengan pornoaksi ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ef5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_answer(context, question):\n",
    "    \n",
    "    answer_pipeline = pipeline_qa(question, context)['answer']\n",
    "    answer_forward_no_grad = forward_qa(question, context)\n",
    "    answer_forward_grad = forward_qa_grad(question, context)\n",
    "    \n",
    "    print(f\"CONTEXT:\")\n",
    "    print(context)\n",
    "    print()\n",
    "    \n",
    "    print(f\"QUESTION:\")\n",
    "    print(question)\n",
    "    print()\n",
    "    \n",
    "    print(\"This answer is from PIPELINE:\")\n",
    "    print(answer_pipeline)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(\"This answer is from MODEL FORWARD NO grad:\")\n",
    "    print(answer_forward_no_grad)\n",
    "    print()\n",
    "    \n",
    "    print(\"This answer is from MODEL FORWARD WITH grad:\")\n",
    "    print(answer_forward_grad)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db463b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:\n",
      "Mikail (Arab: ميكائيل) adalah malaikat yang mengatur air, menurunkan hujan/petir, membagikan rezeki pada manusia, tumbuh-tumbuhan juga hewan-hewan dan lain-lain di muka bumi ini. Dikatakan setiap satu makhluk yang memerlukan rezeki untuk hidup di dunia ini akan diselia rezekinya oleh satu malaikat Karubiyyuun.\n",
      "\n",
      "QUESTION:\n",
      "Apa tugas Malaikat Mikhael di luar Islam?\n",
      "\n",
      "This answer is from PIPELINE:\n",
      "Mikail\n",
      "\n",
      "This answer is from MODEL FORWARD NO grad:\n",
      "\n",
      "\n",
      "This answer is from MODEL FORWARD WITH grad:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_answer(context1, question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
